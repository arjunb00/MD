{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunb00/MD/blob/main/Concatenating_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GHZ6fVt7OBM",
        "outputId": "a8d601ac-607d-48eb-c6ee-4f1320b3e2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['NEV', 'BCC', 'ACK', 'SEK', 'SCC', 'MEL']\n",
            "['NEV', 'BCC', 'ACK', 'SEK', 'SCC', 'MEL']\n",
            "['NEV', 'BCC', 'ACK', 'SEK', 'SCC', 'MEL']\n",
            "Importing libraries completed.\n",
            "Preparing Training Dataset Completed.\n",
            "Preparing Training Dataset Completed.\n",
            "PAT_1249_862_658.png\n",
            "PAT_1304_3507_308.png\n",
            "PAT_1597_2666_130.png\n",
            "PAT_1588_2629_296.png\n",
            "PAT_1639_2879_744.png\n",
            "PAT_1648_2904_523.png\n",
            "PAT_1671_3008_622.png\n",
            "PAT_1698_3122_83.png\n",
            "PAT_1735_3242_27.png\n",
            "PAT_1945_3923_428.png\n",
            "PAT_1984_4038_265.png\n",
            "PAT_2013_4148_303.png\n",
            "PAT_213_328_117.png\n",
            "PAT_2144_4738_120.png\n",
            "PAT_2161_4818_501.png\n",
            "PAT_244_374_726.png\n",
            "PAT_341_715_180.png\n",
            "PAT_344_718_206.png\n",
            "PAT_417_828_723.png\n",
            "PAT_439_859_283.png\n",
            "PAT_471_909_344.png\n",
            "PAT_471_909_394.png\n",
            "PAT_621_1182_456.png\n",
            "PAT_621_1183_56.png\n",
            "PAT_672_1272_705.png\n",
            "PAT_756_1496_480.png\n",
            "PAT_793_1512_327.png\n",
            "PAT_892_1691_881.png\n",
            "PAT_900_1711_622.png\n",
            "PAT_1535_1812_660.png\n",
            "PAT_1514_1760_35.png\n",
            "PAT_1591_2648_723.png\n",
            "PAT_522_3168_747.png\n",
            "PAT_61_299_74.png\n",
            "PAT_1801_3459_857.png\n",
            "PAT_1658_2933_322.png\n",
            "PAT_286_1459_666.png\n",
            "PAT_2062_4397_39.png\n",
            "PAT_1603_2704_969.png\n",
            "PAT_1242_826_903.png\n",
            "PAT_1186_680_578.png\n",
            "PAT_2088_4525_507.png\n",
            "PAT_1881_3717_785.png\n",
            "PAT_1484_1682_240.png\n",
            "PAT_1334_1178_413.png\n",
            "PAT_1107_427_352.png\n",
            "PAT_1503_1728_65.png\n",
            "PAT_1305_1082_88.png\n",
            "PAT_2129_4680_110.png\n",
            "PAT_1364_1246_447.png\n",
            "PAT_115_175_283.png\n",
            "PAT_145_216_158.png\n",
            "PAT_145_216_924.png\n",
            "PAT_152_230_707.png\n",
            "PAT_15_23_240.png\n",
            "PAT_166_257_586.png\n",
            "PAT_242_369_930.png\n",
            "PAT_247_1294_292.png\n",
            "PAT_282_433_934.png\n",
            "PAT_290_445_686.png\n",
            "PAT_301_649_533.png\n",
            "PAT_316_674_924.png\n",
            "PAT_357_737_77.png\n",
            "PAT_385_783_22.png\n",
            "PAT_381_775_566.png\n",
            "PAT_396_796_394.png\n",
            "PAT_406_809_581.png\n",
            "PAT_406_1542_754.png\n",
            "PAT_453_883_719.png\n",
            "PAT_481_923_186.png\n",
            "PAT_507_954_812.png\n",
            "PAT_526_991_109.png\n",
            "PAT_580_1104_684.png\n",
            "PAT_619_1175_767.png\n",
            "PAT_63_99_736.png\n",
            "PAT_645_1300_463.png\n",
            "PAT_666_1268_870.png\n",
            "PAT_662_1251_380.png\n",
            "PAT_681_1299_458.png\n",
            "PAT_690_1309_217.png\n",
            "PAT_736_1392_562.png\n",
            "PAT_73_111_435.png\n",
            "PAT_727_1373_718.png\n",
            "PAT_738_1397_966.png\n",
            "PAT_745_1406_409.png\n",
            "PAT_753_1428_451.png\n",
            "PAT_747_1408_427.png\n",
            "PAT_760_1456_878.png\n",
            "PAT_764_1448_499.png\n",
            "PAT_778_1471_835.png\n",
            "PAT_771_1491_390.png\n",
            "PAT_78_118_164.png\n",
            "PAT_778_1471_911.png\n",
            "PAT_78_118_685.png\n",
            "PAT_806_1519_527.png\n",
            "PAT_815_1536_963.png\n",
            "PAT_823_1558_67.png\n",
            "PAT_842_1606_720.png\n",
            "PAT_838_1587_73.png\n",
            "PAT_842_1606_971.png\n",
            "PAT_845_1604_313.png\n",
            "PAT_874_1671_176.png\n",
            "PAT_926_1758_714.png\n",
            "PAT_938_1790_977.png\n",
            "PAT_985_1856_91.png\n",
            "PAT_974_1841_693.png\n",
            "PAT_530_999_696.png\n",
            "PAT_691_1311_2.png\n",
            "PAT_716_1345_460.png\n",
            "PAT_268_413_643.png\n",
            "PAT_946_1809_687.png\n",
            "PAT_759_1538_566.png\n",
            "PAT_737_1394_936.png\n",
            "PAT_682_1292_642.png\n",
            "PAT_355_732_102.png\n",
            "PAT_79_120_532.png\n",
            "PAT_153_233_45.png\n",
            "PAT_830_1564_740.png\n",
            "PAT_970_1833_270.png\n",
            "PAT_67_104_730.png\n",
            "PAT_948_1801_993.png\n",
            "PAT_264_1368_868.png\n",
            "PAT_587_1117_880.png\n",
            "PAT_217_332_906.png\n",
            "PAT_361_743_82.png\n",
            "PAT_326_690_797.png\n",
            "PAT_430_848_476.png\n",
            "PAT_480_921_1.png\n",
            "PAT_903_1719_943.png\n",
            "PAT_198_302_44.png\n",
            "PAT_430_847_315.png\n",
            "PAT_147_219_206.png\n",
            "PAT_837_1583_124.png\n",
            "PAT_458_891_709.png\n",
            "PAT_363_746_18.png\n",
            "PAT_645_1223_443.png\n",
            "PAT_193_297_295.png\n",
            "PAT_981_1848_486.png\n",
            "PAT_295_639_240.png\n",
            "PAT_483_925_716.png\n",
            "PAT_464_900_548.png\n",
            "PAT_669_1265_76.png\n",
            "PAT_154_236_800.png\n",
            "PAT_899_1702_32.png\n",
            "PAT_523_988_900.png\n",
            "PAT_559_1093_774.png\n",
            "PAT_134_201_587.png\n",
            "PAT_758_1441_891.png\n",
            "PAT_55_84_506.png\n",
            "PAT_224_343_782.png\n",
            "PAT_30_41_815.png\n",
            "PAT_835_1578_592.png\n",
            "PAT_354_731_155.png\n",
            "PAT_251_382_240.png\n",
            "PAT_272_419_906.png\n",
            "PAT_866_1649_674.png\n",
            "PAT_118_180_500.png\n",
            "PAT_161_250_338.png\n",
            "PAT_877_1667_118.png\n",
            "PAT_338_707_774.png\n",
            "PAT_822_1551_642.png\n",
            "PAT_906_1723_560.png\n",
            "PAT_409_812_974.png\n",
            "PAT_547_1033_911.png\n",
            "PAT_160_249_76.png\n",
            "PAT_118_180_386.png\n",
            "PAT_691_1310_780.png\n",
            "PAT_645_1223_124.png\n",
            "PAT_255_388_77.png\n",
            "PAT_510_959_985.png\n",
            "PAT_782_1492_890.png\n",
            "PAT_413_819_550.png\n",
            "PAT_822_1550_826.png\n",
            "PAT_950_1804_64.png\n",
            "PAT_457_889_346.png\n",
            "PAT_123_187_164.png\n",
            "PAT_720_1362_700.png\n",
            "PAT_817_1537_943.png\n",
            "PAT_464_899_646.png\n",
            "PAT_554_1051_301.png\n",
            "PAT_964_1821_83.png\n",
            "PAT_876_1665_386.png\n",
            "PAT_814_1532_265.png\n",
            "PAT_154_235_162.png\n",
            "PAT_53_1144_892.png\n",
            "PAT_455_885_115.png\n",
            "PAT_208_314_741.png\n",
            "PAT_522_986_800.png\n",
            "PAT_660_1249_469.png\n",
            "PAT_79_120_739.png\n",
            "PAT_551_1047_567.png\n",
            "PAT_516_966_57.png\n",
            "PAT_789_1483_793.png\n",
            "PAT_473_911_630.png\n",
            "PAT_261_401_911.png\n",
            "PAT_598_1137_888.png\n",
            "PAT_499_945_878.png\n",
            "PAT_650_1241_329.png\n",
            "PAT_156_241_538.png\n",
            "PAT_699_1320_901.png\n",
            "PAT_142_210_904.png\n",
            "PAT_744_1405_742.png\n",
            "PAT_461_895_72.png\n",
            "PAT_477_915_389.png\n",
            "PAT_168_262_74.png\n",
            "PAT_305_655_30.png\n",
            "PAT_90_138_369.png\n",
            "PAT_39_55_233.png\n",
            "PAT_677_1348_51.png\n",
            "PAT_897_1710_793.png\n",
            "PAT_565_1073_749.png\n",
            "PAT_362_744_283.png\n",
            "PAT_980_1851_351.png\n",
            "PAT_279_429_142.png\n",
            "PAT_813_1619_543.png\n",
            "PAT_683_1296_237.png\n",
            "PAT_527_1360_225.png\n",
            "PAT_251_382_164.png\n",
            "PAT_452_882_896.png\n",
            "PAT_1026_124_206.png\n",
            "PAT_1031_148_292.png\n",
            "PAT_1031_148_29.png\n",
            "PAT_1067_284_533.png\n",
            "PAT_1065_277_768.png\n",
            "PAT_1070_305_907.png\n",
            "PAT_1061_254_813.png\n",
            "PAT_1066_281_99.png\n",
            "PAT_1074_322_662.png\n",
            "PAT_1093_379_273.png\n",
            "PAT_108_424_944.png\n",
            "PAT_1125_492_523.png\n",
            "PAT_1119_472_513.png\n",
            "PAT_1128_497_322.png\n",
            "PAT_1145_534_529.png\n",
            "PAT_1260_894_63.png\n",
            "PAT_1282_978_328.png\n",
            "PAT_1316_1114_517.png\n",
            "PAT_1347_1226_928.png\n",
            "PAT_1402_1376_507.png\n",
            "PAT_1402_1376_160.png\n",
            "PAT_1393_1355_270.png\n",
            "PAT_1406_1397_669.png\n",
            "PAT_1410_1410_46.png\n",
            "PAT_1418_1449_580.png\n",
            "PAT_1411_1414_355.png\n",
            "PAT_1427_1485_119.png\n",
            "PAT_1432_1497_328.png\n",
            "PAT_1441_1531_670.png\n",
            "PAT_1469_1627_109.png\n",
            "PAT_1464_1612_284.png\n",
            "PAT_1483_1678_850.png\n",
            "PAT_1479_1669_376.png\n",
            "PAT_1498_1716_626.png\n",
            "PAT_1531_1800_683.png\n",
            "PAT_1557_2399_681.png\n",
            "PAT_1572_2495_335.png\n",
            "PAT_1572_2494_224.png\n",
            "PAT_1575_2516_23.png\n",
            "PAT_158_621_68.png\n",
            "PAT_1593_2651_122.png\n",
            "PAT_1596_2664_323.png\n",
            "PAT_1673_3011_440.png\n",
            "PAT_1689_3075_38.png\n",
            "PAT_1736_3246_166.png\n",
            "PAT_1725_3222_943.png\n",
            "PAT_1739_3261_325.png\n",
            "PAT_1726_3228_674.png\n",
            "PAT_1728_3234_404.png\n",
            "PAT_1760_3326_525.png\n",
            "PAT_1794_3435_711.png\n",
            "PAT_1790_3427_723.png\n",
            "PAT_1850_3631_519.png\n",
            "PAT_1833_3599_827.png\n",
            "PAT_1822_3576_145.png\n",
            "PAT_1854_3637_323.png\n",
            "PAT_1881_3711_217.png\n",
            "PAT_1899_3779_475.png\n",
            "PAT_1976_4000_701.png\n",
            "PAT_2011_4140_787.png\n",
            "PAT_2009_4134_828.png\n",
            "PAT_2010_4136_318.png\n",
            "PAT_2024_4225_170.png\n",
            "PAT_2043_4308_681.png\n",
            "PAT_207_3951_140.png\n",
            "PAT_2077_4463_77.png\n",
            "PAT_2082_4491_519.png\n",
            "PAT_2093_4542_192.png\n",
            "PAT_2115_4613_260.png\n",
            "PAT_2107_4595_381.png\n",
            "PAT_2157_4789_969.png\n",
            "PAT_217_963_806.png\n",
            "PAT_236_1071_810.png\n",
            "PAT_286_1458_381.png\n",
            "PAT_302_1633_845.png\n",
            "PAT_401_4594_970.png\n",
            "PAT_441_2868_663.png\n",
            "PAT_493_3016_512.png\n",
            "PAT_528_3072_615.png\n",
            "PAT_526_3205_611.png\n",
            "PAT_531_3079_652.png\n",
            "PAT_587_3431_6.png\n",
            "PAT_639_3674_452.png\n",
            "PAT_673_3928_888.png\n",
            "PAT_698_1150_881.png\n",
            "PAT_71_145_971.png\n",
            "PAT_728_1533_952.png\n",
            "PAT_742_1470_425.png\n",
            "PAT_817_4471_179.png\n",
            "PAT_999_20_401.png\n",
            "PAT_85_129_225.png\n",
            "PAT_747_1409_466.png\n",
            "PAT_873_1661_178.png\n",
            "PAT_457_890_255.png\n",
            "PAT_69_1053_540.png\n",
            "PAT_914_1740_932.png\n",
            "PAT_57_90_910.png\n",
            "PAT_38_1003_226.png\n",
            "PAT_38_1003_68.png\n",
            "PAT_862_1643_386.png\n",
            "PAT_224_1340_142.png\n",
            "PAT_65_101_847.png\n",
            "PAT_749_1414_770.png\n",
            "PAT_277_427_658.png\n",
            "PAT_880_1675_704.png\n",
            "PAT_973_1839_34.png\n",
            "PAT_749_1415_164.png\n",
            "PAT_81_123_52.png\n",
            "PAT_753_1427_496.png\n",
            "PAT_920_1748_942.png\n",
            "PAT_802_1516_510.png\n",
            "PAT_931_1763_617.png\n",
            "PAT_492_1784_50.png\n",
            "PAT_849_1613_824.png\n",
            "PAT_522_985_991.png\n",
            "PAT_886_1684_103.png\n",
            "PAT_437_857_425.png\n",
            "PAT_585_1130_552.png\n",
            "PAT_224_1340_551.png\n",
            "PAT_802_1516_910.png\n",
            "PAT_992_1864_653.png\n",
            "PAT_868_1657_698.png\n",
            "PAT_202_308_721.png\n",
            "PAT_412_818_61.png\n",
            "PAT_126_192_208.png\n",
            "PAT_742_1402_818.png\n",
            "PAT_961_1818_992.png\n",
            "PAT_857_1628_916.png\n",
            "PAT_492_1782_752.png\n",
            "PAT_971_1847_99.png\n",
            "PAT_71_108_56.png\n",
            "PAT_108_162_526.png\n",
            "PAT_1840_3612_959.png\n",
            "PAT_1449_1554_932.png\n",
            "PAT_388_2452_548.png\n",
            "PAT_1112_455_845.png\n",
            "PAT_798_1772_469.png\n",
            "PAT_1906_3798_588.png\n",
            "PAT_2150_4766_754.png\n",
            "PAT_1088_373_781.png\n",
            "PAT_2089_4529_762.png\n",
            "PAT_1042_187_746.png\n",
            "PAT_1983_4032_502.png\n",
            "PAT_328_1738_890.png\n",
            "PAT_2020_4176_865.png\n",
            "PAT_1376_1297_686.png\n",
            "PAT_1641_2886_284.png\n",
            "PAT_1198_715_648.png\n",
            "PAT_1286_999_824.png\n",
            "PAT_1340_1208_931.png\n",
            "PAT_1418_1447_360.png\n",
            "PAT_1577_2543_510.png\n",
            "PAT_1585_2619_396.png\n",
            "PAT_1709_3159_144.png\n",
            "PAT_1838_3609_254.png\n",
            "PAT_1902_3790_168.png\n",
            "PAT_1988_4052_897.png\n",
            "PAT_2016_4156_941.png\n",
            "PAT_2049_4348_485.png\n",
            "PAT_2046_4323_762.png\n",
            "PAT_2109_4598_112.png\n",
            "PAT_2113_4607_329.png\n",
            "PAT_267_1258_693.png\n",
            "PAT_354_1814_153.png\n",
            "PAT_373_2598_235.png\n",
            "PAT_701_4056_457.png\n",
            "PAT_819_1541_727.png\n",
            "PAT_616_1169_3.png\n",
            "PAT_1569_2456_393.png\n",
            "PAT_1755_3309_828.png\n",
            "PAT_1975_3999_453.png\n",
            "PAT_1400_1372_276.png\n",
            "PAT_2065_4412_407.png\n",
            "PAT_851_4520_743.png\n",
            "PAT_1582_2574_989.png\n",
            "PAT_1554_4170_992.png\n",
            "PAT_1946_3924_627.png\n",
            "PAT_1068_285_845.png\n",
            "PAT_93_361_467.png\n",
            "PAT_1593_2652_422.png\n",
            "PAT_1783_3413_850.png\n",
            "PAT_1978_4004_304.png\n",
            "PAT_1494_1707_125.png\n",
            "PAT_1495_1708_278.png\n",
            "PAT_1871_3666_675.png\n",
            "PAT_1942_3917_379.png\n",
            "PAT_59_46_537.png\n",
            "PAT_1341_1212_539.png\n",
            "PAT_1618_2771_628.png\n",
            "PAT_1509_1741_810.png\n",
            "PAT_1550_1888_216.png\n",
            "PAT_1555_2396_71.png\n",
            "PAT_1304_1081_84.png\n",
            "PAT_881_1681_462.png\n",
            "PAT_338_709_306.png\n",
            "PAT_714_1337_709.png\n",
            "PAT_953_1806_326.png\n",
            "PAT_296_640_206.png\n",
            "PAT_787_1482_328.png\n",
            "PAT_482_924_134.png\n",
            "PAT_935_1786_482.png\n",
            "PAT_487_930_973.png\n",
            "PAT_338_710_319.png\n",
            "PAT_686_1355_168.png\n",
            "PAT_574_1081_628.png\n",
            "PAT_271_418_14.png\n",
            "PAT_834_1574_276.png\n",
            "PAT_805_1517_647.png\n",
            "PAT_834_1572_430.png\n",
            "PAT_859_1638_598.png\n",
            "PAT_637_1217_274.png\n",
            "PAT_48_74_309.png\n",
            "PAT_809_1527_902.png\n",
            "PAT_771_1489_345.png\n",
            "PAT_383_777_129.png\n",
            "PAT_390_790_505.png\n",
            "PAT_570_1084_939.png\n",
            "PAT_49_75_326.png\n",
            "PAT_113_172_610.png\n",
            "PAT_500_947_77.png\n",
            "PAT_56_86_802.png\n",
            "PAT_527_1359_165.png\n",
            "PAT_365_750_854.png\n",
            "PAT_978_1844_715.png\n",
            "PAT_561_1069_418.png\n",
            "PAT_945_1796_583.png\n",
            "PAT_605_1152_536.png\n",
            "PAT_51_78_959.png\n",
            "PAT_492_937_126.png\n",
            "PAT_215_330_224.png\n",
            "PAT_882_1677_789.png\n",
            "PAT_844_1605_308.png\n",
            "PAT_614_1166_885.png\n",
            "PAT_801_1518_78.png\n",
            "PAT_320_681_410.png\n",
            "PAT_656_1246_489.png\n",
            "PAT_972_1843_756.png\n",
            "PAT_890_1693_904.png\n",
            "PAT_746_1407_708.png\n",
            "PAT_949_1802_582.png\n",
            "PAT_656_1246_483.png\n",
            "PAT_890_1693_454.png\n",
            "PAT_801_1518_574.png\n",
            "Preparing Training Dataset Completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('gdrive/My Drive/PAD-UFES-20/output') \n",
        "\n",
        "\n",
        "\n",
        "# listing the folders containing images\n",
        "\n",
        "# Train Dataset\n",
        "train_folder=\"train/\"\n",
        "class_names=os.listdir(train_folder)\n",
        "print(class_names)\n",
        "\n",
        "# Validation Dataset\n",
        "validation_folder=\"val/\"\n",
        "val_class_names=os.listdir(validation_folder)\n",
        "print(val_class_names)\n",
        "\n",
        "# Validation Dataset\n",
        "test_folder=\"test/\"\n",
        "test_class_names=os.listdir(test_folder)\n",
        "print(val_class_names)\n",
        "\n",
        "# importing libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Libraries for TensorFlow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Library for Transfer Learning\n",
        "from tensorflow.keras.applications import resnet50\n",
        "\n",
        "print(\"Importing libraries completed.\")\n",
        "\n",
        "\n",
        "# Preparing Training image data (image array and class name) for processing\n",
        "\n",
        "# Declaring variables\n",
        "trainX=[] # to store array value of the images\n",
        "trainY=[] # to store the labels of the images\n",
        "\n",
        "\n",
        "\n",
        "for folder in os.listdir(train_folder):\n",
        "    image_list=os.listdir(train_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "        # Loading images\n",
        "        img=image.load_img(train_folder+\"/\"+folder+\"/\"+img_name,target_size=(128,128))\n",
        "        \n",
        "        # Converting to arrary\n",
        "        img=image.img_to_array(img)\n",
        "        \n",
        "        # Transfer Learning: this is to apply preprocess of resnet to our images before passing it to resnet\n",
        "        img=resnet50.preprocess_input(img) #  Optional step\n",
        "        \n",
        "        # Appending the arrarys\n",
        "        trainX.append(img) # appending image array\n",
        "        trainY.append(class_names.index(folder)) # appending class index to the array\n",
        "        \n",
        "print(\"Preparing Training Dataset Completed.\")\n",
        "\n",
        "# Preparing Training image data (image array and class name) for processing\n",
        "\n",
        "# Declaring variables\n",
        "valX=[] # to store array value of the images\n",
        "valY=[] # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(validation_folder):\n",
        "    image_list=os.listdir(validation_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "        # Loading images\n",
        "        img=image.load_img(validation_folder+\"/\"+folder+\"/\"+img_name,target_size=(128,128))\n",
        "        \n",
        "        # Converting to arrary\n",
        "        img=image.img_to_array(img)\n",
        "        \n",
        "        # Transfer Learning: this is to apply preprocess of resnet to our images before passing it to resnet\n",
        "        img=resnet50.preprocess_input(img) #  Optional step\n",
        "        \n",
        "        # Appending the arrarys\n",
        "        valX.append(img) # appending image array\n",
        "        valY.append(class_names.index(folder)) # appending class index to the array\n",
        "        \n",
        "print(\"Preparing Training Dataset Completed.\")\n",
        "\n",
        "# Preparing Training image data (image array and class name) for processing\n",
        "\n",
        "# Declaring variables\n",
        "testX=[] # to store array value of the images\n",
        "testY=[] # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(test_folder):\n",
        "    image_list=os.listdir(test_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "        # Loading images\n",
        "        print(img_name)\n",
        "        img=image.load_img(test_folder+\"/\"+folder+\"/\"+img_name,target_size=(128,128))\n",
        "        \n",
        "        # Converting to arrary\n",
        "        img=image.img_to_array(img)\n",
        "        \n",
        "        # Transfer Learning: this is to apply preprocess of resnet to our images before passing it to resnet\n",
        "        img=resnet50.preprocess_input(img) #  Optional step\n",
        "        \n",
        "        # Appending the arrarys\n",
        "        testX.append(img) # appending image array\n",
        "        testY.append(class_names.index(folder)) # appending class index to the array\n",
        "        \n",
        "print(\"Preparing Training Dataset Completed.\")\n",
        "\n",
        "trainX = np.array(trainX)\n",
        "trainY = np.array(trainY)\n",
        "valX = np.array(valX)\n",
        "valY = np.array(valY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hasfGquhBngv"
      },
      "outputs": [],
      "source": [
        "os.chdir('../')\n",
        "metadata = pd.read_csv('metadata.csv')\n",
        "metadata.drop(labels=['diagnostic', 'patient_id', 'background_father', 'background_mother', 'pesticide', 'has_piped_water', 'has_sewage_system', 'biopsed', 'fitspatrick', 'diameter_1', 'diameter_2', 'lesion_id'], axis=1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "844PVk5zCZKu"
      },
      "outputs": [],
      "source": [
        "def encode_and_bind(original_dataframe, feature_to_encode):\n",
        "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "    dummies.add_suffix(feature_to_encode + '_')\n",
        "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    res.drop(feature_to_encode, axis = 1, inplace = True)\n",
        "    return(res)\n",
        "\n",
        "\n",
        "features_to_encode = ['itch', 'grew', 'hurt', 'changed', 'bleed', 'region', 'skin_cancer_history', 'cancer_history', 'gender', 'smoke', 'drink','elevation']\n",
        "\n",
        "for feature in features_to_encode :\n",
        "    metadata = encode_and_bind(metadata, feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiC3kmaIEfmx",
        "outputId": "f52dddc0-8b6f-4d06-b89f-2f5491987ef1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(metadata.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6lFcR2sE58d",
        "outputId": "33c13e1d-61ca-41dc-a4a4-34a064ce3d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing Training Dataset Completed.\n"
          ]
        }
      ],
      "source": [
        "os.chdir('./output')\n",
        "trainXtab=[] # to store array value of the images\n",
        "trainYtab=[] # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(train_folder):\n",
        "    image_list=os.listdir(train_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "        trainXtab.append(img_name) # appending image array\n",
        "\n",
        "        \n",
        "valXtab=[] # to store array value of the images\n",
        "valYtab=[] # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(validation_folder):\n",
        "    image_list=os.listdir(validation_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "        valXtab.append(img_name) \n",
        "\n",
        "        \n",
        "print(\"Preparing Training Dataset Completed.\")\n",
        "\n",
        "# Preparing Training image data (image array and class name) for processing\n",
        "\n",
        "# Declaring variables\n",
        "testXtab=[] # to store array value of the images\n",
        "testYtab=[] # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(test_folder):\n",
        "    image_list=os.listdir(test_folder+\"/\"+folder)\n",
        "    for img_name in image_list:\n",
        "\n",
        "        testXtab.append(img_name) \n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "cZoSgVfUHI73",
        "outputId": "ec22539f-29cf-4464-8d44-9f881cbab0af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70747a8b-7fdf-4049-a996-b32b69f1baa0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>img_id</th>\n",
              "      <th>itch_False</th>\n",
              "      <th>itch_True</th>\n",
              "      <th>itch_UNK</th>\n",
              "      <th>grew_False</th>\n",
              "      <th>grew_True</th>\n",
              "      <th>grew_UNK</th>\n",
              "      <th>hurt_False</th>\n",
              "      <th>hurt_True</th>\n",
              "      <th>...</th>\n",
              "      <th>cancer_history_True</th>\n",
              "      <th>gender_FEMALE</th>\n",
              "      <th>gender_MALE</th>\n",
              "      <th>smoke_False</th>\n",
              "      <th>smoke_True</th>\n",
              "      <th>drink_False</th>\n",
              "      <th>drink_True</th>\n",
              "      <th>elevation_False</th>\n",
              "      <th>elevation_True</th>\n",
              "      <th>elevation_UNK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>PAT_1516_1765_530.png</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "      <td>PAT_46_881_939.png</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>77</td>\n",
              "      <td>PAT_1545_1867_547.png</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>75</td>\n",
              "      <td>PAT_1989_4061_934.png</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>79</td>\n",
              "      <td>PAT_684_1302_588.png</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 44 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70747a8b-7fdf-4049-a996-b32b69f1baa0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70747a8b-7fdf-4049-a996-b32b69f1baa0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70747a8b-7fdf-4049-a996-b32b69f1baa0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   age                 img_id  itch_False  itch_True  itch_UNK  grew_False  \\\n",
              "0    8  PAT_1516_1765_530.png           1          0         0           1   \n",
              "1   55     PAT_46_881_939.png           0          1         0           0   \n",
              "2   77  PAT_1545_1867_547.png           0          1         0           1   \n",
              "3   75  PAT_1989_4061_934.png           0          1         0           1   \n",
              "4   79   PAT_684_1302_588.png           0          1         0           0   \n",
              "\n",
              "   grew_True  grew_UNK  hurt_False  hurt_True  ...  cancer_history_True  \\\n",
              "0          0         0           1          0  ...                    0   \n",
              "1          1         0           1          0  ...                    1   \n",
              "2          0         0           1          0  ...                    0   \n",
              "3          0         0           1          0  ...                    0   \n",
              "4          1         0           1          0  ...                    0   \n",
              "\n",
              "   gender_FEMALE  gender_MALE  smoke_False  smoke_True  drink_False  \\\n",
              "0              0            0            0           0            0   \n",
              "1              1            0            1           0            1   \n",
              "2              0            0            0           0            0   \n",
              "3              0            0            0           0            0   \n",
              "4              0            1            1           0            0   \n",
              "\n",
              "   drink_True  elevation_False  elevation_True  elevation_UNK  \n",
              "0           0                1               0              0  \n",
              "1           0                0               1              0  \n",
              "2           0                1               0              0  \n",
              "3           0                1               0              0  \n",
              "4           1                0               1              0  \n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY2BcV3ZGj-F"
      },
      "outputs": [],
      "source": [
        "trainXmetadata = pd.DataFrame()\n",
        "valXmetadata = pd.DataFrame()\n",
        "testXmetadata = pd.DataFrame()\n",
        "\n",
        "for i in trainXtab:\n",
        "  trainXmetadata = trainXmetadata.append(metadata[metadata['img_id'] == i])\n",
        "\n",
        "for i in testXtab:\n",
        "  testXmetadata = testXmetadata.append(metadata[metadata['img_id'] == i])\n",
        "\n",
        "for i in valXtab:\n",
        "  valXmetadata = valXmetadata.append(metadata[metadata['img_id'] == i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwzbMlooIHzK",
        "outputId": "2708701a-d93d-41b8-c0f0-0b6671660ac3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "462"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(testXtab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6T09VYBJBqG"
      },
      "outputs": [],
      "source": [
        "trainXmetadata.drop(labels=['img_id'], axis=1, inplace = True)\n",
        "testXmetadata.drop(labels=['img_id'], axis=1, inplace = True)\n",
        "valXmetadata.drop(labels=['img_id'], axis=1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y6qc0CrKHxq",
        "outputId": "84a67bd8-e6fa-4d7a-fd55-de82bbbf3ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainXmetadata.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkxRLPo6X_i3",
        "outputId": "88a4739d-a39a-4e22-c376-a387d4ba9241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "458"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(valXmetadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e__1S5ltFIIn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import imutils as imutils\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf # machine learning\n",
        "from tqdm import tqdm # make your loops show a smart progress meter \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "from keras.layers import Input, Dense, Concatenate\n",
        "from keras.models import Model\n",
        "# from keras.applications import VGG16\n",
        "import numpy as np\n",
        "\n",
        "# Some random images, labels and target label\n",
        "images = trainX\n",
        "trainLabels = trainXmetadata\n",
        "trainTarget = trainY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "trainFeatures = model.predict(images)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSmGghfiNdu5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDAt47sHSuB3"
      },
      "outputs": [],
      "source": [
        "images = valX\n",
        "valLabels = valXmetadata\n",
        "valTarget = valY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "valFeatures = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeOMuqyWFIMY",
        "outputId": "895c0d0b-6eb3-4329-8e8f-2652411985b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " feature_input (InputLayer)     [(None, 8192)]       0           []                               \n",
            "                                                                                                  \n",
            " label_input (InputLayer)       [(None, 43)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 8235)         0           ['feature_input[0][0]',          \n",
            "                                                                  'label_input[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          1054208     ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 32)           0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 6)            198         ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,064,742\n",
            "Trainable params: 1,064,742\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# Two input layers: one for the image features, one for additional labels\n",
        "feature_input = Input((8192,), name='feature_input')\n",
        "label_input = Input((43, ), name='label_input')\n",
        "\n",
        "# Concatenate the features\n",
        "concatenate_layer = Concatenate(name='concatenation')([feature_input, label_input]) \n",
        "dense = Dense(256,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.5)(dense)\n",
        "dense = Dense(128,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.5)(dense)\n",
        "dense = Dense(64,activation='relu')(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "dense = Dense(32,activation='relu')(dense)\n",
        "flatten=layers.Flatten()(dense) \n",
        "\n",
        "output = Dense(6, name='output_layer', activation='softmax')(flatten)\n",
        "\n",
        "# To define the model, pass list of input layers\n",
        "model = Model(inputs=[feature_input, label_input], outputs=output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fe16Wh8X18S"
      },
      "outputs": [],
      "source": [
        "valFeatures = np.reshape(valFeatures, (458, -1))\n",
        "trainFeatures = np.reshape(trainFeatures, (1378, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gCecn_wYTL6",
        "outputId": "b80422f1-b324-44f5-df0b-a83cffdccc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(458, 8192) (1378, 8192) (1378, 43) (458, 8192) (458, 43)\n"
          ]
        }
      ],
      "source": [
        "print(valFeatures.shape, trainFeatures.shape, trainLabels.shape,valFeatures.shape,valLabels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmaztURwTiux",
        "outputId": "cf6c7cf0-3df4-4ae9-cb38-55bcc71c8a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 7.7549 - accuracy: 0.2859 - val_loss: 3.0887 - val_accuracy: 0.2664\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 6.3889 - accuracy: 0.2794 - val_loss: 2.9158 - val_accuracy: 0.2576\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 5.7997 - accuracy: 0.2787 - val_loss: 2.6937 - val_accuracy: 0.2707\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 5.1388 - accuracy: 0.2823 - val_loss: 2.5319 - val_accuracy: 0.2664\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 4.5799 - accuracy: 0.2939 - val_loss: 2.4103 - val_accuracy: 0.2729\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 4.4577 - accuracy: 0.2874 - val_loss: 2.2745 - val_accuracy: 0.2686\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 4.1678 - accuracy: 0.3019 - val_loss: 2.1447 - val_accuracy: 0.2882\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.8484 - accuracy: 0.3033 - val_loss: 2.0892 - val_accuracy: 0.2860\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.5487 - accuracy: 0.3077 - val_loss: 2.0221 - val_accuracy: 0.2838\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.4947 - accuracy: 0.3019 - val_loss: 1.9749 - val_accuracy: 0.2860\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.2445 - accuracy: 0.3208 - val_loss: 1.9375 - val_accuracy: 0.2904\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.0391 - accuracy: 0.3033 - val_loss: 1.8863 - val_accuracy: 0.2991\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 3.1224 - accuracy: 0.3055 - val_loss: 1.8437 - val_accuracy: 0.3079\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.8482 - accuracy: 0.3367 - val_loss: 1.8100 - val_accuracy: 0.3188\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.6775 - accuracy: 0.3208 - val_loss: 1.7993 - val_accuracy: 0.3253\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 0s 9ms/step - loss: 2.7053 - accuracy: 0.3157 - val_loss: 1.7799 - val_accuracy: 0.3210\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.5610 - accuracy: 0.3316 - val_loss: 1.7650 - val_accuracy: 0.3210\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.4469 - accuracy: 0.3324 - val_loss: 1.7466 - val_accuracy: 0.3231\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.3899 - accuracy: 0.3353 - val_loss: 1.7248 - val_accuracy: 0.3297\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.3154 - accuracy: 0.3447 - val_loss: 1.7095 - val_accuracy: 0.3362\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.2406 - accuracy: 0.3374 - val_loss: 1.6985 - val_accuracy: 0.3341\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.1814 - accuracy: 0.3563 - val_loss: 1.6882 - val_accuracy: 0.3472\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.2309 - accuracy: 0.3454 - val_loss: 1.6844 - val_accuracy: 0.3581\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.0363 - accuracy: 0.3476 - val_loss: 1.6827 - val_accuracy: 0.3559\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.1299 - accuracy: 0.3469 - val_loss: 1.6722 - val_accuracy: 0.3581\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.0554 - accuracy: 0.3345 - val_loss: 1.6682 - val_accuracy: 0.3690\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.0677 - accuracy: 0.3520 - val_loss: 1.6629 - val_accuracy: 0.3559\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.9611 - accuracy: 0.3549 - val_loss: 1.6592 - val_accuracy: 0.3668\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 2.0153 - accuracy: 0.3505 - val_loss: 1.6491 - val_accuracy: 0.3777\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.9581 - accuracy: 0.3512 - val_loss: 1.6440 - val_accuracy: 0.3821\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.8760 - accuracy: 0.3708 - val_loss: 1.6400 - val_accuracy: 0.3843\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.8148 - accuracy: 0.3810 - val_loss: 1.6404 - val_accuracy: 0.3843\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7819 - accuracy: 0.3723 - val_loss: 1.6365 - val_accuracy: 0.3777\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7758 - accuracy: 0.3875 - val_loss: 1.6281 - val_accuracy: 0.3930\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.8097 - accuracy: 0.3737 - val_loss: 1.6255 - val_accuracy: 0.3908\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7245 - accuracy: 0.3824 - val_loss: 1.6201 - val_accuracy: 0.3930\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.8066 - accuracy: 0.3672 - val_loss: 1.6178 - val_accuracy: 0.4017\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7555 - accuracy: 0.3788 - val_loss: 1.6134 - val_accuracy: 0.4061\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7327 - accuracy: 0.3824 - val_loss: 1.6119 - val_accuracy: 0.4083\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.7503 - accuracy: 0.3650 - val_loss: 1.6095 - val_accuracy: 0.4039\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.6439 - accuracy: 0.3977 - val_loss: 1.6095 - val_accuracy: 0.4127\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.6560 - accuracy: 0.3962 - val_loss: 1.6065 - val_accuracy: 0.4192\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.6068 - accuracy: 0.4013 - val_loss: 1.5998 - val_accuracy: 0.4192\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.5742 - accuracy: 0.4231 - val_loss: 1.5976 - val_accuracy: 0.4192\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.5934 - accuracy: 0.4071 - val_loss: 1.5971 - val_accuracy: 0.4279\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.6229 - accuracy: 0.4158 - val_loss: 1.5940 - val_accuracy: 0.4148\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.5900 - accuracy: 0.4057 - val_loss: 1.5941 - val_accuracy: 0.4105\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.5977 - accuracy: 0.4028 - val_loss: 1.5970 - val_accuracy: 0.4214\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.5615 - accuracy: 0.4071 - val_loss: 1.5959 - val_accuracy: 0.4236\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.5205 - accuracy: 0.4194 - val_loss: 1.5929 - val_accuracy: 0.4301\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 1.5763 - accuracy: 0.4151 - val_loss: 1.5863 - val_accuracy: 0.4301\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.5455 - accuracy: 0.4282 - val_loss: 1.5848 - val_accuracy: 0.4323\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.5191 - accuracy: 0.4325 - val_loss: 1.5828 - val_accuracy: 0.4258\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.5041 - accuracy: 0.4245 - val_loss: 1.5816 - val_accuracy: 0.4389\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.4925 - accuracy: 0.4296 - val_loss: 1.5775 - val_accuracy: 0.4258\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4680 - accuracy: 0.4412 - val_loss: 1.5763 - val_accuracy: 0.4279\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.4861 - accuracy: 0.4463 - val_loss: 1.5738 - val_accuracy: 0.4389\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4618 - accuracy: 0.4325 - val_loss: 1.5711 - val_accuracy: 0.4389\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.4773 - accuracy: 0.4303 - val_loss: 1.5653 - val_accuracy: 0.4279\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.5004 - accuracy: 0.4311 - val_loss: 1.5610 - val_accuracy: 0.4367\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4372 - accuracy: 0.4332 - val_loss: 1.5596 - val_accuracy: 0.4367\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.4479 - accuracy: 0.4659 - val_loss: 1.5582 - val_accuracy: 0.4454\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4669 - accuracy: 0.4419 - val_loss: 1.5588 - val_accuracy: 0.4454\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4496 - accuracy: 0.4390 - val_loss: 1.5527 - val_accuracy: 0.4498\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.4159 - accuracy: 0.4463 - val_loss: 1.5473 - val_accuracy: 0.4520\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3945 - accuracy: 0.4456 - val_loss: 1.5422 - val_accuracy: 0.4716\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.3938 - accuracy: 0.4463 - val_loss: 1.5399 - val_accuracy: 0.4738\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3850 - accuracy: 0.4376 - val_loss: 1.5354 - val_accuracy: 0.4672\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3793 - accuracy: 0.4731 - val_loss: 1.5352 - val_accuracy: 0.4651\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3735 - accuracy: 0.4644 - val_loss: 1.5351 - val_accuracy: 0.4585\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3599 - accuracy: 0.4739 - val_loss: 1.5337 - val_accuracy: 0.4694\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3754 - accuracy: 0.4666 - val_loss: 1.5338 - val_accuracy: 0.4782\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3560 - accuracy: 0.4594 - val_loss: 1.5288 - val_accuracy: 0.4825\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3020 - accuracy: 0.4927 - val_loss: 1.5250 - val_accuracy: 0.4782\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3318 - accuracy: 0.4804 - val_loss: 1.5225 - val_accuracy: 0.4847\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3811 - accuracy: 0.4601 - val_loss: 1.5239 - val_accuracy: 0.4694\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.3659 - accuracy: 0.4586 - val_loss: 1.5225 - val_accuracy: 0.4803\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3614 - accuracy: 0.4586 - val_loss: 1.5200 - val_accuracy: 0.4716\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3025 - accuracy: 0.5000 - val_loss: 1.5146 - val_accuracy: 0.4694\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3085 - accuracy: 0.4877 - val_loss: 1.5128 - val_accuracy: 0.4760\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2954 - accuracy: 0.4804 - val_loss: 1.5073 - val_accuracy: 0.4738\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3247 - accuracy: 0.4695 - val_loss: 1.5021 - val_accuracy: 0.4825\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.3483 - accuracy: 0.4717 - val_loss: 1.5077 - val_accuracy: 0.4869\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2689 - accuracy: 0.4971 - val_loss: 1.5069 - val_accuracy: 0.4869\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2513 - accuracy: 0.5087 - val_loss: 1.5002 - val_accuracy: 0.4956\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2850 - accuracy: 0.4848 - val_loss: 1.5021 - val_accuracy: 0.4913\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2457 - accuracy: 0.4993 - val_loss: 1.5026 - val_accuracy: 0.4760\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2333 - accuracy: 0.5232 - val_loss: 1.4967 - val_accuracy: 0.4760\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2588 - accuracy: 0.5160 - val_loss: 1.4987 - val_accuracy: 0.4803\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.2750 - accuracy: 0.4855 - val_loss: 1.4998 - val_accuracy: 0.4869\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2469 - accuracy: 0.5036 - val_loss: 1.4988 - val_accuracy: 0.4869\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2395 - accuracy: 0.5203 - val_loss: 1.4940 - val_accuracy: 0.4913\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2706 - accuracy: 0.5029 - val_loss: 1.4978 - val_accuracy: 0.4891\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2415 - accuracy: 0.5116 - val_loss: 1.4959 - val_accuracy: 0.4891\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2203 - accuracy: 0.5029 - val_loss: 1.4952 - val_accuracy: 0.4891\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2252 - accuracy: 0.5116 - val_loss: 1.4935 - val_accuracy: 0.4869\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2250 - accuracy: 0.5203 - val_loss: 1.4906 - val_accuracy: 0.4760\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2313 - accuracy: 0.5036 - val_loss: 1.4881 - val_accuracy: 0.4847\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 0s 9ms/step - loss: 1.2253 - accuracy: 0.4985 - val_loss: 1.4850 - val_accuracy: 0.4760\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2003 - accuracy: 0.5298 - val_loss: 1.4866 - val_accuracy: 0.4738\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.2000 - accuracy: 0.5145 - val_loss: 1.4870 - val_accuracy: 0.4760\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1757 - accuracy: 0.5196 - val_loss: 1.4852 - val_accuracy: 0.4760\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1811 - accuracy: 0.5290 - val_loss: 1.4805 - val_accuracy: 0.4803\n",
            "Epoch 104/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1764 - accuracy: 0.5196 - val_loss: 1.4774 - val_accuracy: 0.4869\n",
            "Epoch 105/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1613 - accuracy: 0.5348 - val_loss: 1.4736 - val_accuracy: 0.4825\n",
            "Epoch 106/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1502 - accuracy: 0.5428 - val_loss: 1.4763 - val_accuracy: 0.4825\n",
            "Epoch 107/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1514 - accuracy: 0.5356 - val_loss: 1.4733 - val_accuracy: 0.4934\n",
            "Epoch 108/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1731 - accuracy: 0.5225 - val_loss: 1.4712 - val_accuracy: 0.4913\n",
            "Epoch 109/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1584 - accuracy: 0.5283 - val_loss: 1.4699 - val_accuracy: 0.5000\n",
            "Epoch 110/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1710 - accuracy: 0.5312 - val_loss: 1.4697 - val_accuracy: 0.4934\n",
            "Epoch 111/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1116 - accuracy: 0.5450 - val_loss: 1.4696 - val_accuracy: 0.4891\n",
            "Epoch 112/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.1740 - accuracy: 0.5254 - val_loss: 1.4684 - val_accuracy: 0.4869\n",
            "Epoch 113/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.1531 - accuracy: 0.5254 - val_loss: 1.4628 - val_accuracy: 0.4891\n",
            "Epoch 114/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1203 - accuracy: 0.5385 - val_loss: 1.4638 - val_accuracy: 0.4934\n",
            "Epoch 115/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1223 - accuracy: 0.5406 - val_loss: 1.4640 - val_accuracy: 0.4934\n",
            "Epoch 116/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.1227 - accuracy: 0.5530 - val_loss: 1.4644 - val_accuracy: 0.4956\n",
            "Epoch 117/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.1024 - accuracy: 0.5581 - val_loss: 1.4623 - val_accuracy: 0.4847\n",
            "Epoch 118/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0949 - accuracy: 0.5472 - val_loss: 1.4581 - val_accuracy: 0.4891\n",
            "Epoch 119/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.1106 - accuracy: 0.5348 - val_loss: 1.4549 - val_accuracy: 0.4869\n",
            "Epoch 120/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.0837 - accuracy: 0.5668 - val_loss: 1.4509 - val_accuracy: 0.4913\n",
            "Epoch 121/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.0884 - accuracy: 0.5377 - val_loss: 1.4515 - val_accuracy: 0.4934\n",
            "Epoch 122/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.0772 - accuracy: 0.5552 - val_loss: 1.4518 - val_accuracy: 0.5022\n",
            "Epoch 123/200\n",
            "44/44 [==============================] - 0s 10ms/step - loss: 1.0741 - accuracy: 0.5508 - val_loss: 1.4508 - val_accuracy: 0.5000\n",
            "Epoch 124/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.1150 - accuracy: 0.5450 - val_loss: 1.4511 - val_accuracy: 0.4956\n",
            "Epoch 125/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0843 - accuracy: 0.5457 - val_loss: 1.4436 - val_accuracy: 0.5044\n",
            "Epoch 126/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0877 - accuracy: 0.5450 - val_loss: 1.4417 - val_accuracy: 0.5000\n",
            "Epoch 127/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0604 - accuracy: 0.5755 - val_loss: 1.4414 - val_accuracy: 0.4978\n",
            "Epoch 128/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0623 - accuracy: 0.5711 - val_loss: 1.4436 - val_accuracy: 0.5087\n",
            "Epoch 129/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0609 - accuracy: 0.5697 - val_loss: 1.4416 - val_accuracy: 0.5000\n",
            "Epoch 130/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0329 - accuracy: 0.5762 - val_loss: 1.4395 - val_accuracy: 0.5109\n",
            "Epoch 131/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0705 - accuracy: 0.5559 - val_loss: 1.4378 - val_accuracy: 0.5109\n",
            "Epoch 132/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0160 - accuracy: 0.5820 - val_loss: 1.4356 - val_accuracy: 0.5197\n",
            "Epoch 133/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 1.0451 - accuracy: 0.5682 - val_loss: 1.4349 - val_accuracy: 0.5218\n",
            "Epoch 134/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.0346 - accuracy: 0.5689 - val_loss: 1.4366 - val_accuracy: 0.5175\n",
            "Epoch 135/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0387 - accuracy: 0.5617 - val_loss: 1.4375 - val_accuracy: 0.5109\n",
            "Epoch 136/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0313 - accuracy: 0.5769 - val_loss: 1.4336 - val_accuracy: 0.5175\n",
            "Epoch 137/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0100 - accuracy: 0.5900 - val_loss: 1.4330 - val_accuracy: 0.5197\n",
            "Epoch 138/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0074 - accuracy: 0.5769 - val_loss: 1.4316 - val_accuracy: 0.5087\n",
            "Epoch 139/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 1.0136 - accuracy: 0.5791 - val_loss: 1.4327 - val_accuracy: 0.5218\n",
            "Epoch 140/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 1.0336 - accuracy: 0.5595 - val_loss: 1.4348 - val_accuracy: 0.5153\n",
            "Epoch 141/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 1.0250 - accuracy: 0.5718 - val_loss: 1.4383 - val_accuracy: 0.5153\n",
            "Epoch 142/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0171 - accuracy: 0.5668 - val_loss: 1.4373 - val_accuracy: 0.5175\n",
            "Epoch 143/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9853 - accuracy: 0.5784 - val_loss: 1.4336 - val_accuracy: 0.5197\n",
            "Epoch 144/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9787 - accuracy: 0.5929 - val_loss: 1.4327 - val_accuracy: 0.5131\n",
            "Epoch 145/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9780 - accuracy: 0.5936 - val_loss: 1.4341 - val_accuracy: 0.5197\n",
            "Epoch 146/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9809 - accuracy: 0.5827 - val_loss: 1.4392 - val_accuracy: 0.5197\n",
            "Epoch 147/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9515 - accuracy: 0.6154 - val_loss: 1.4387 - val_accuracy: 0.5153\n",
            "Epoch 148/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 1.0186 - accuracy: 0.5784 - val_loss: 1.4353 - val_accuracy: 0.5131\n",
            "Epoch 149/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9884 - accuracy: 0.5820 - val_loss: 1.4322 - val_accuracy: 0.5175\n",
            "Epoch 150/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9752 - accuracy: 0.5878 - val_loss: 1.4341 - val_accuracy: 0.5197\n",
            "Epoch 151/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9823 - accuracy: 0.5711 - val_loss: 1.4332 - val_accuracy: 0.5087\n",
            "Epoch 152/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9625 - accuracy: 0.5994 - val_loss: 1.4336 - val_accuracy: 0.5131\n",
            "Epoch 153/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9798 - accuracy: 0.5893 - val_loss: 1.4317 - val_accuracy: 0.5218\n",
            "Epoch 154/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9602 - accuracy: 0.5972 - val_loss: 1.4330 - val_accuracy: 0.5131\n",
            "Epoch 155/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9614 - accuracy: 0.5994 - val_loss: 1.4340 - val_accuracy: 0.5153\n",
            "Epoch 156/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9825 - accuracy: 0.5878 - val_loss: 1.4352 - val_accuracy: 0.5175\n",
            "Epoch 157/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9420 - accuracy: 0.5994 - val_loss: 1.4367 - val_accuracy: 0.5153\n",
            "Epoch 158/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9600 - accuracy: 0.5849 - val_loss: 1.4364 - val_accuracy: 0.5197\n",
            "Epoch 159/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9696 - accuracy: 0.5864 - val_loss: 1.4356 - val_accuracy: 0.5153\n",
            "Epoch 160/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9363 - accuracy: 0.6016 - val_loss: 1.4307 - val_accuracy: 0.5175\n",
            "Epoch 161/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9411 - accuracy: 0.5972 - val_loss: 1.4298 - val_accuracy: 0.5131\n",
            "Epoch 162/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9241 - accuracy: 0.5951 - val_loss: 1.4379 - val_accuracy: 0.5175\n",
            "Epoch 163/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9271 - accuracy: 0.6052 - val_loss: 1.4390 - val_accuracy: 0.5240\n",
            "Epoch 164/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9392 - accuracy: 0.5965 - val_loss: 1.4380 - val_accuracy: 0.5197\n",
            "Epoch 165/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9528 - accuracy: 0.5856 - val_loss: 1.4402 - val_accuracy: 0.5218\n",
            "Epoch 166/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9284 - accuracy: 0.6009 - val_loss: 1.4412 - val_accuracy: 0.5218\n",
            "Epoch 167/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9289 - accuracy: 0.6060 - val_loss: 1.4430 - val_accuracy: 0.5262\n",
            "Epoch 168/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9288 - accuracy: 0.6074 - val_loss: 1.4369 - val_accuracy: 0.5328\n",
            "Epoch 169/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.9151 - accuracy: 0.6154 - val_loss: 1.4378 - val_accuracy: 0.5218\n",
            "Epoch 170/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.9122 - accuracy: 0.6060 - val_loss: 1.4334 - val_accuracy: 0.5240\n",
            "Epoch 171/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8897 - accuracy: 0.6219 - val_loss: 1.4355 - val_accuracy: 0.5131\n",
            "Epoch 172/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8846 - accuracy: 0.6299 - val_loss: 1.4400 - val_accuracy: 0.5262\n",
            "Epoch 173/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8988 - accuracy: 0.6190 - val_loss: 1.4376 - val_accuracy: 0.5284\n",
            "Epoch 174/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8780 - accuracy: 0.6408 - val_loss: 1.4319 - val_accuracy: 0.5240\n",
            "Epoch 175/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.9040 - accuracy: 0.6190 - val_loss: 1.4432 - val_accuracy: 0.5262\n",
            "Epoch 176/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.8493 - accuracy: 0.6567 - val_loss: 1.4414 - val_accuracy: 0.5240\n",
            "Epoch 177/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8977 - accuracy: 0.6205 - val_loss: 1.4420 - val_accuracy: 0.5218\n",
            "Epoch 178/200\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.8753 - accuracy: 0.6422 - val_loss: 1.4491 - val_accuracy: 0.5240\n",
            "Epoch 179/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8747 - accuracy: 0.6509 - val_loss: 1.4514 - val_accuracy: 0.5218\n",
            "Epoch 180/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8856 - accuracy: 0.6321 - val_loss: 1.4498 - val_accuracy: 0.5328\n",
            "Epoch 181/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8586 - accuracy: 0.6575 - val_loss: 1.4468 - val_accuracy: 0.5306\n",
            "Epoch 182/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8513 - accuracy: 0.6480 - val_loss: 1.4473 - val_accuracy: 0.5349\n",
            "Epoch 183/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.8785 - accuracy: 0.6364 - val_loss: 1.4502 - val_accuracy: 0.5218\n",
            "Epoch 184/200\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.8449 - accuracy: 0.6459 - val_loss: 1.4529 - val_accuracy: 0.5284\n",
            "Epoch 185/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8539 - accuracy: 0.6567 - val_loss: 1.4479 - val_accuracy: 0.5328\n",
            "Epoch 186/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8729 - accuracy: 0.6422 - val_loss: 1.4487 - val_accuracy: 0.5306\n",
            "Epoch 187/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.8525 - accuracy: 0.6466 - val_loss: 1.4537 - val_accuracy: 0.5284\n",
            "Epoch 188/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8809 - accuracy: 0.6321 - val_loss: 1.4499 - val_accuracy: 0.5415\n",
            "Epoch 189/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.8555 - accuracy: 0.6517 - val_loss: 1.4501 - val_accuracy: 0.5371\n",
            "Epoch 190/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8285 - accuracy: 0.6727 - val_loss: 1.4479 - val_accuracy: 0.5306\n",
            "Epoch 191/200\n",
            "44/44 [==============================] - 1s 11ms/step - loss: 0.8453 - accuracy: 0.6538 - val_loss: 1.4495 - val_accuracy: 0.5328\n",
            "Epoch 192/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8268 - accuracy: 0.6800 - val_loss: 1.4444 - val_accuracy: 0.5371\n",
            "Epoch 193/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8088 - accuracy: 0.6763 - val_loss: 1.4515 - val_accuracy: 0.5328\n",
            "Epoch 194/200\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.8441 - accuracy: 0.6495 - val_loss: 1.4527 - val_accuracy: 0.5284\n",
            "Epoch 195/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8177 - accuracy: 0.6662 - val_loss: 1.4577 - val_accuracy: 0.5306\n",
            "Epoch 196/200\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.8078 - accuracy: 0.6582 - val_loss: 1.4580 - val_accuracy: 0.5393\n",
            "Epoch 197/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8159 - accuracy: 0.6763 - val_loss: 1.4558 - val_accuracy: 0.5328\n",
            "Epoch 198/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8246 - accuracy: 0.6720 - val_loss: 1.4624 - val_accuracy: 0.5349\n",
            "Epoch 199/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8138 - accuracy: 0.6669 - val_loss: 1.4641 - val_accuracy: 0.5371\n",
            "Epoch 200/200\n",
            "44/44 [==============================] - 1s 12ms/step - loss: 0.8165 - accuracy: 0.6792 - val_loss: 1.4629 - val_accuracy: 0.5328\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f919b1184d0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.00001)\n",
        "\n",
        "model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "valconc = [valFeatures, valLabels]\n",
        "\n",
        "valData=(valconc ,valTarget)\n",
        "\n",
        "# To fit the model, pass a list of inputs arrays\n",
        "model.fit(x=[trainFeatures, trainLabels], y=trainTarget, epochs=200,batch_size=32,verbose=True, validation_data = valData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30kGFC4_mMdv"
      },
      "source": [
        "### Trying with ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io27fka0bVnR"
      },
      "outputs": [],
      "source": [
        "# we are using resnet50 for transfer learnin here. So we have imported it\n",
        "from tensorflow.keras.applications import resnet50\n",
        "\n",
        "# Some random images, labels and target label\n",
        "images = trainX\n",
        "trainLabels = trainXmetadata\n",
        "trainTarget = trainY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = resnet50.ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "trainFeatures = model.predict(images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiMsW5wkmL8P"
      },
      "outputs": [],
      "source": [
        "images = valX\n",
        "valLabels = valXmetadata\n",
        "valTarget = valY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = resnet50.ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "valFeatures = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAoWdQh9meHB",
        "outputId": "5c916854-656a-4b9b-e438-f6369249c753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " feature_input (InputLayer)     [(None, 32768)]      0           []                               \n",
            "                                                                                                  \n",
            " label_input (InputLayer)       [(None, 43)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 32811)        0           ['feature_input[0][0]',          \n",
            "                                                                  'label_input[0][0]']            \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 128)          4199936     ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 64)           8256        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 64)           0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 32)           2080        ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 32)           0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 6)            198         ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,210,470\n",
            "Trainable params: 4,210,470\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# Two input layers: one for the image features, one for additional labels\n",
        "feature_input = Input((32768,), name='feature_input')\n",
        "label_input = Input((43, ), name='label_input')\n",
        "\n",
        "concatenate_layer = Concatenate(name='concatenation')([feature_input, label_input]) \n",
        "dense = Dense(1024,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.6)(dense)\n",
        "dense = Dense(512,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.6)(dense)\n",
        "dense = Dense(256,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.6)(dense)\n",
        "dense = Dense(128,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.6)(dense)\n",
        "dense = Dense(64,activation='relu')(dense)\n",
        "dense = Dropout(0.6)(dense)\n",
        "dense = Dense(32,activation='relu')(dense)\n",
        "flatten=layers.Flatten()(dense) \n",
        "\n",
        "\n",
        "output = Dense(6, name='output_layer', activation='softmax')(flatten)\n",
        "\n",
        "# To define the model, pass list of input layers\n",
        "model = Model(inputs=[feature_input, label_input], outputs=output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOp9itX-n5cV",
        "outputId": "1facb45d-a99c-4bfc-e6c3-7ced24d7996e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.1084 - accuracy: 0.2286 - val_loss: 1.8228 - val_accuracy: 0.2445\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 3.3376 - accuracy: 0.1959 - val_loss: 1.8287 - val_accuracy: 0.2118\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 3.0512 - accuracy: 0.2003 - val_loss: 1.8382 - val_accuracy: 0.1594\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 2.6737 - accuracy: 0.1967 - val_loss: 1.8312 - val_accuracy: 0.1507\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 2.4482 - accuracy: 0.2083 - val_loss: 1.8161 - val_accuracy: 0.1485\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 2.3544 - accuracy: 0.1974 - val_loss: 1.8001 - val_accuracy: 0.1528\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 2.0773 - accuracy: 0.2097 - val_loss: 1.7831 - val_accuracy: 0.1616\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 2.0534 - accuracy: 0.2119 - val_loss: 1.7660 - val_accuracy: 0.1594\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 2.0197 - accuracy: 0.2097 - val_loss: 1.7495 - val_accuracy: 0.1965\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8729 - accuracy: 0.2192 - val_loss: 1.7366 - val_accuracy: 0.2358\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.8940 - accuracy: 0.2003 - val_loss: 1.7272 - val_accuracy: 0.2555\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8208 - accuracy: 0.2533 - val_loss: 1.7183 - val_accuracy: 0.2576\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7709 - accuracy: 0.2293 - val_loss: 1.7125 - val_accuracy: 0.2620\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7971 - accuracy: 0.2344 - val_loss: 1.7068 - val_accuracy: 0.2860\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7753 - accuracy: 0.2772 - val_loss: 1.7034 - val_accuracy: 0.3013\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7588 - accuracy: 0.2337 - val_loss: 1.7003 - val_accuracy: 0.3144\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.8237 - accuracy: 0.2446 - val_loss: 1.6999 - val_accuracy: 0.3319\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7270 - accuracy: 0.2496 - val_loss: 1.6989 - val_accuracy: 0.3035\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7625 - accuracy: 0.2446 - val_loss: 1.6986 - val_accuracy: 0.3035\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.7540 - accuracy: 0.2583 - val_loss: 1.7000 - val_accuracy: 0.3166\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.6905 - accuracy: 0.2446 - val_loss: 1.7011 - val_accuracy: 0.3253\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7249 - accuracy: 0.2620 - val_loss: 1.7012 - val_accuracy: 0.3253\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.6898 - accuracy: 0.2707 - val_loss: 1.7003 - val_accuracy: 0.3100\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6921 - accuracy: 0.2663 - val_loss: 1.6993 - val_accuracy: 0.3166\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.6891 - accuracy: 0.2547 - val_loss: 1.6980 - val_accuracy: 0.3166\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.6712 - accuracy: 0.2446 - val_loss: 1.6970 - val_accuracy: 0.3122\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7108 - accuracy: 0.2511 - val_loss: 1.6946 - val_accuracy: 0.3275\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.6720 - accuracy: 0.2591 - val_loss: 1.6910 - val_accuracy: 0.3253\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.6668 - accuracy: 0.2845 - val_loss: 1.6871 - val_accuracy: 0.3341\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.6654 - accuracy: 0.2997 - val_loss: 1.6825 - val_accuracy: 0.3406\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6563 - accuracy: 0.2830 - val_loss: 1.6773 - val_accuracy: 0.3384\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.6605 - accuracy: 0.2808 - val_loss: 1.6715 - val_accuracy: 0.3493\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.6243 - accuracy: 0.2975 - val_loss: 1.6666 - val_accuracy: 0.3493\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.6680 - accuracy: 0.2823 - val_loss: 1.6620 - val_accuracy: 0.3515\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.6366 - accuracy: 0.2881 - val_loss: 1.6584 - val_accuracy: 0.3668\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.6598 - accuracy: 0.2990 - val_loss: 1.6544 - val_accuracy: 0.3734\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6433 - accuracy: 0.2939 - val_loss: 1.6508 - val_accuracy: 0.3734\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.6096 - accuracy: 0.3171 - val_loss: 1.6479 - val_accuracy: 0.3712\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.6798 - accuracy: 0.2910 - val_loss: 1.6428 - val_accuracy: 0.3865\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.6205 - accuracy: 0.3091 - val_loss: 1.6368 - val_accuracy: 0.3886\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.5983 - accuracy: 0.3208 - val_loss: 1.6329 - val_accuracy: 0.3755\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7020 - accuracy: 0.2859 - val_loss: 1.6315 - val_accuracy: 0.3755\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.6404 - accuracy: 0.3106 - val_loss: 1.6296 - val_accuracy: 0.3755\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.5693 - accuracy: 0.3222 - val_loss: 1.6286 - val_accuracy: 0.3734\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.5937 - accuracy: 0.3193 - val_loss: 1.6267 - val_accuracy: 0.3777\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.6291 - accuracy: 0.3309 - val_loss: 1.6247 - val_accuracy: 0.3734\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.5915 - accuracy: 0.3055 - val_loss: 1.6226 - val_accuracy: 0.3690\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.5901 - accuracy: 0.3070 - val_loss: 1.6208 - val_accuracy: 0.3712\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.5771 - accuracy: 0.3273 - val_loss: 1.6198 - val_accuracy: 0.3668\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.5950 - accuracy: 0.3106 - val_loss: 1.6185 - val_accuracy: 0.3690\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.5793 - accuracy: 0.3309 - val_loss: 1.6182 - val_accuracy: 0.3799\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.6131 - accuracy: 0.2939 - val_loss: 1.6170 - val_accuracy: 0.3734\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.5483 - accuracy: 0.3447 - val_loss: 1.6148 - val_accuracy: 0.3799\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.5423 - accuracy: 0.3091 - val_loss: 1.6122 - val_accuracy: 0.3886\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.5663 - accuracy: 0.3280 - val_loss: 1.6081 - val_accuracy: 0.3974\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.5822 - accuracy: 0.3396 - val_loss: 1.6054 - val_accuracy: 0.3974\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.5227 - accuracy: 0.3360 - val_loss: 1.6015 - val_accuracy: 0.4017\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.5368 - accuracy: 0.3498 - val_loss: 1.5983 - val_accuracy: 0.4039\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.5493 - accuracy: 0.3345 - val_loss: 1.5951 - val_accuracy: 0.4017\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.5228 - accuracy: 0.3433 - val_loss: 1.5924 - val_accuracy: 0.3996\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.5620 - accuracy: 0.3389 - val_loss: 1.5916 - val_accuracy: 0.3930\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.5170 - accuracy: 0.3520 - val_loss: 1.5904 - val_accuracy: 0.3865\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.5596 - accuracy: 0.3200 - val_loss: 1.5916 - val_accuracy: 0.3843\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.5025 - accuracy: 0.3585 - val_loss: 1.5924 - val_accuracy: 0.3843\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.4806 - accuracy: 0.3701 - val_loss: 1.5905 - val_accuracy: 0.3821\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.5224 - accuracy: 0.3374 - val_loss: 1.5879 - val_accuracy: 0.3930\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.5317 - accuracy: 0.3411 - val_loss: 1.5845 - val_accuracy: 0.3930\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.5321 - accuracy: 0.3462 - val_loss: 1.5800 - val_accuracy: 0.3930\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.5110 - accuracy: 0.3462 - val_loss: 1.5738 - val_accuracy: 0.3996\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.5238 - accuracy: 0.3483 - val_loss: 1.5689 - val_accuracy: 0.4039\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.4563 - accuracy: 0.3614 - val_loss: 1.5642 - val_accuracy: 0.4105\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.4871 - accuracy: 0.3541 - val_loss: 1.5580 - val_accuracy: 0.4192\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.4922 - accuracy: 0.3476 - val_loss: 1.5526 - val_accuracy: 0.4323\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.4676 - accuracy: 0.3578 - val_loss: 1.5489 - val_accuracy: 0.4432\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.4849 - accuracy: 0.3672 - val_loss: 1.5460 - val_accuracy: 0.4498\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.4606 - accuracy: 0.3716 - val_loss: 1.5446 - val_accuracy: 0.4541\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.4674 - accuracy: 0.3592 - val_loss: 1.5437 - val_accuracy: 0.4498\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.4857 - accuracy: 0.3701 - val_loss: 1.5415 - val_accuracy: 0.4410\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.4352 - accuracy: 0.3716 - val_loss: 1.5386 - val_accuracy: 0.4432\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.4757 - accuracy: 0.3672 - val_loss: 1.5349 - val_accuracy: 0.4585\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.4480 - accuracy: 0.3810 - val_loss: 1.5302 - val_accuracy: 0.4520\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.4667 - accuracy: 0.3781 - val_loss: 1.5245 - val_accuracy: 0.4563\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.4688 - accuracy: 0.3774 - val_loss: 1.5196 - val_accuracy: 0.4607\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.4263 - accuracy: 0.3759 - val_loss: 1.5152 - val_accuracy: 0.4563\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.4662 - accuracy: 0.3759 - val_loss: 1.5118 - val_accuracy: 0.4629\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.4136 - accuracy: 0.3868 - val_loss: 1.5091 - val_accuracy: 0.4629\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.4405 - accuracy: 0.4078 - val_loss: 1.5064 - val_accuracy: 0.4629\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.4257 - accuracy: 0.3882 - val_loss: 1.5052 - val_accuracy: 0.4694\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.3967 - accuracy: 0.3810 - val_loss: 1.5028 - val_accuracy: 0.4672\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.3941 - accuracy: 0.4100 - val_loss: 1.4985 - val_accuracy: 0.4607\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.4421 - accuracy: 0.4049 - val_loss: 1.4915 - val_accuracy: 0.4694\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.4349 - accuracy: 0.4078 - val_loss: 1.4847 - val_accuracy: 0.4782\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.4051 - accuracy: 0.4332 - val_loss: 1.4775 - val_accuracy: 0.4803\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.4071 - accuracy: 0.4216 - val_loss: 1.4685 - val_accuracy: 0.4803\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.4246 - accuracy: 0.4013 - val_loss: 1.4594 - val_accuracy: 0.4891\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.4287 - accuracy: 0.3955 - val_loss: 1.4529 - val_accuracy: 0.4869\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.4286 - accuracy: 0.4100 - val_loss: 1.4497 - val_accuracy: 0.4934\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.3877 - accuracy: 0.4354 - val_loss: 1.4478 - val_accuracy: 0.4913\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.3897 - accuracy: 0.4231 - val_loss: 1.4480 - val_accuracy: 0.4825\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.3938 - accuracy: 0.4340 - val_loss: 1.4497 - val_accuracy: 0.4913\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.3899 - accuracy: 0.4202 - val_loss: 1.4504 - val_accuracy: 0.4891\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.3620 - accuracy: 0.4187 - val_loss: 1.4498 - val_accuracy: 0.4847\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.3869 - accuracy: 0.4194 - val_loss: 1.4476 - val_accuracy: 0.5044\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.3761 - accuracy: 0.4187 - val_loss: 1.4450 - val_accuracy: 0.4956\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.4385 - accuracy: 0.4042 - val_loss: 1.4411 - val_accuracy: 0.5000\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.3885 - accuracy: 0.4289 - val_loss: 1.4365 - val_accuracy: 0.5044\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.3828 - accuracy: 0.4405 - val_loss: 1.4322 - val_accuracy: 0.5109\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.3666 - accuracy: 0.4216 - val_loss: 1.4290 - val_accuracy: 0.5175\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.4147 - accuracy: 0.4086 - val_loss: 1.4279 - val_accuracy: 0.5131\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.3577 - accuracy: 0.4347 - val_loss: 1.4257 - val_accuracy: 0.5153\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.3367 - accuracy: 0.4427 - val_loss: 1.4229 - val_accuracy: 0.5197\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.3431 - accuracy: 0.4492 - val_loss: 1.4196 - val_accuracy: 0.5197\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.3374 - accuracy: 0.4470 - val_loss: 1.4159 - val_accuracy: 0.5218\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.3687 - accuracy: 0.4507 - val_loss: 1.4121 - val_accuracy: 0.5262\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.3827 - accuracy: 0.4158 - val_loss: 1.4094 - val_accuracy: 0.5218\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.3086 - accuracy: 0.4412 - val_loss: 1.4067 - val_accuracy: 0.5284\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.2622 - accuracy: 0.4470 - val_loss: 1.4021 - val_accuracy: 0.5284\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.4100 - accuracy: 0.4194 - val_loss: 1.3983 - val_accuracy: 0.5284\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.3198 - accuracy: 0.4448 - val_loss: 1.3978 - val_accuracy: 0.5284\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.3264 - accuracy: 0.4898 - val_loss: 1.3948 - val_accuracy: 0.5306\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.3148 - accuracy: 0.4586 - val_loss: 1.3919 - val_accuracy: 0.5328\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.3249 - accuracy: 0.4739 - val_loss: 1.3902 - val_accuracy: 0.5349\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.3265 - accuracy: 0.4536 - val_loss: 1.3920 - val_accuracy: 0.5371\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.3165 - accuracy: 0.4666 - val_loss: 1.3921 - val_accuracy: 0.5459\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.3135 - accuracy: 0.4456 - val_loss: 1.3922 - val_accuracy: 0.5437\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.3314 - accuracy: 0.4724 - val_loss: 1.3895 - val_accuracy: 0.5349\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.3378 - accuracy: 0.4528 - val_loss: 1.3867 - val_accuracy: 0.5328\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.2985 - accuracy: 0.4782 - val_loss: 1.3835 - val_accuracy: 0.5306\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.2833 - accuracy: 0.4920 - val_loss: 1.3771 - val_accuracy: 0.5371\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.3142 - accuracy: 0.4688 - val_loss: 1.3693 - val_accuracy: 0.5437\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.2939 - accuracy: 0.4761 - val_loss: 1.3611 - val_accuracy: 0.5437\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.3282 - accuracy: 0.4543 - val_loss: 1.3533 - val_accuracy: 0.5546\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.3145 - accuracy: 0.4492 - val_loss: 1.3462 - val_accuracy: 0.5546\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.3215 - accuracy: 0.4666 - val_loss: 1.3423 - val_accuracy: 0.5546\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.2733 - accuracy: 0.4659 - val_loss: 1.3419 - val_accuracy: 0.5546\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.2867 - accuracy: 0.4594 - val_loss: 1.3416 - val_accuracy: 0.5568\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.2825 - accuracy: 0.4884 - val_loss: 1.3434 - val_accuracy: 0.5524\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.3245 - accuracy: 0.4717 - val_loss: 1.3463 - val_accuracy: 0.5502\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.2608 - accuracy: 0.4833 - val_loss: 1.3480 - val_accuracy: 0.5437\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.2886 - accuracy: 0.4898 - val_loss: 1.3500 - val_accuracy: 0.5437\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.2421 - accuracy: 0.4775 - val_loss: 1.3509 - val_accuracy: 0.5480\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.2690 - accuracy: 0.4949 - val_loss: 1.3502 - val_accuracy: 0.5480\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.2804 - accuracy: 0.4797 - val_loss: 1.3475 - val_accuracy: 0.5524\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.3004 - accuracy: 0.4775 - val_loss: 1.3444 - val_accuracy: 0.5568\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.3336 - accuracy: 0.4855 - val_loss: 1.3400 - val_accuracy: 0.5590\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.2617 - accuracy: 0.4964 - val_loss: 1.3338 - val_accuracy: 0.5611\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.3241 - accuracy: 0.4688 - val_loss: 1.3279 - val_accuracy: 0.5699\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.2706 - accuracy: 0.4985 - val_loss: 1.3214 - val_accuracy: 0.5742\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.2200 - accuracy: 0.5189 - val_loss: 1.3156 - val_accuracy: 0.5808\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.2199 - accuracy: 0.5102 - val_loss: 1.3069 - val_accuracy: 0.5808\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.2107 - accuracy: 0.4927 - val_loss: 1.2989 - val_accuracy: 0.5808\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.2523 - accuracy: 0.5058 - val_loss: 1.2922 - val_accuracy: 0.5852\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.2329 - accuracy: 0.5210 - val_loss: 1.2889 - val_accuracy: 0.5852\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.2269 - accuracy: 0.4964 - val_loss: 1.2909 - val_accuracy: 0.5895\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.2466 - accuracy: 0.5087 - val_loss: 1.2938 - val_accuracy: 0.5852\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.2303 - accuracy: 0.5058 - val_loss: 1.2944 - val_accuracy: 0.5830\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.2372 - accuracy: 0.5065 - val_loss: 1.2951 - val_accuracy: 0.5786\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.2923 - accuracy: 0.4898 - val_loss: 1.2957 - val_accuracy: 0.5721\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.1883 - accuracy: 0.5319 - val_loss: 1.2927 - val_accuracy: 0.5786\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.2305 - accuracy: 0.5036 - val_loss: 1.2885 - val_accuracy: 0.5786\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.1735 - accuracy: 0.5290 - val_loss: 1.2836 - val_accuracy: 0.5830\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.2336 - accuracy: 0.5138 - val_loss: 1.2791 - val_accuracy: 0.5808\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.1867 - accuracy: 0.5319 - val_loss: 1.2739 - val_accuracy: 0.5786\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.2424 - accuracy: 0.5022 - val_loss: 1.2703 - val_accuracy: 0.5808\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.2363 - accuracy: 0.5022 - val_loss: 1.2667 - val_accuracy: 0.5895\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.2725 - accuracy: 0.4942 - val_loss: 1.2630 - val_accuracy: 0.5852\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.2674 - accuracy: 0.5109 - val_loss: 1.2650 - val_accuracy: 0.5852\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.2090 - accuracy: 0.5261 - val_loss: 1.2662 - val_accuracy: 0.5764\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.1959 - accuracy: 0.5377 - val_loss: 1.2685 - val_accuracy: 0.5742\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.2577 - accuracy: 0.5073 - val_loss: 1.2715 - val_accuracy: 0.5699\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.2321 - accuracy: 0.5196 - val_loss: 1.2730 - val_accuracy: 0.5721\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.2214 - accuracy: 0.5152 - val_loss: 1.2719 - val_accuracy: 0.5721\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.1968 - accuracy: 0.5370 - val_loss: 1.2671 - val_accuracy: 0.5786\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.2089 - accuracy: 0.5298 - val_loss: 1.2612 - val_accuracy: 0.5786\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.2060 - accuracy: 0.5232 - val_loss: 1.2534 - val_accuracy: 0.5895\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.1682 - accuracy: 0.5261 - val_loss: 1.2459 - val_accuracy: 0.5961\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.1737 - accuracy: 0.5247 - val_loss: 1.2402 - val_accuracy: 0.5961\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.1797 - accuracy: 0.5196 - val_loss: 1.2363 - val_accuracy: 0.5917\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.1864 - accuracy: 0.5305 - val_loss: 1.2347 - val_accuracy: 0.5961\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.2481 - accuracy: 0.5116 - val_loss: 1.2345 - val_accuracy: 0.5939\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.1226 - accuracy: 0.5363 - val_loss: 1.2377 - val_accuracy: 0.5939\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.0831 - accuracy: 0.5406 - val_loss: 1.2414 - val_accuracy: 0.5873\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.1917 - accuracy: 0.5370 - val_loss: 1.2422 - val_accuracy: 0.5873\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.1784 - accuracy: 0.5385 - val_loss: 1.2437 - val_accuracy: 0.5917\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.1655 - accuracy: 0.5472 - val_loss: 1.2431 - val_accuracy: 0.5873\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.1786 - accuracy: 0.5493 - val_loss: 1.2421 - val_accuracy: 0.5852\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.1292 - accuracy: 0.5479 - val_loss: 1.2398 - val_accuracy: 0.5873\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.1961 - accuracy: 0.5646 - val_loss: 1.2365 - val_accuracy: 0.5917\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.1563 - accuracy: 0.5341 - val_loss: 1.2323 - val_accuracy: 0.5895\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.1949 - accuracy: 0.5225 - val_loss: 1.2299 - val_accuracy: 0.5939\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.1272 - accuracy: 0.5385 - val_loss: 1.2253 - val_accuracy: 0.5895\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.1419 - accuracy: 0.5443 - val_loss: 1.2211 - val_accuracy: 0.5939\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.1220 - accuracy: 0.5718 - val_loss: 1.2177 - val_accuracy: 0.6004\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.1042 - accuracy: 0.5624 - val_loss: 1.2147 - val_accuracy: 0.6048\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.0980 - accuracy: 0.5399 - val_loss: 1.2151 - val_accuracy: 0.6004\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.1341 - accuracy: 0.5479 - val_loss: 1.2161 - val_accuracy: 0.6026\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.1095 - accuracy: 0.5682 - val_loss: 1.2169 - val_accuracy: 0.5961\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.1581 - accuracy: 0.5421 - val_loss: 1.2155 - val_accuracy: 0.5983\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.1876 - accuracy: 0.5428 - val_loss: 1.2134 - val_accuracy: 0.5961\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.1078 - accuracy: 0.5493 - val_loss: 1.2116 - val_accuracy: 0.5961\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.1308 - accuracy: 0.5544 - val_loss: 1.2084 - val_accuracy: 0.6004\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.0874 - accuracy: 0.5486 - val_loss: 1.2063 - val_accuracy: 0.6026\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.0820 - accuracy: 0.5595 - val_loss: 1.2035 - val_accuracy: 0.6048\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.1654 - accuracy: 0.5755 - val_loss: 1.2006 - val_accuracy: 0.6048\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.1267 - accuracy: 0.5552 - val_loss: 1.1994 - val_accuracy: 0.6026\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.1232 - accuracy: 0.5624 - val_loss: 1.2003 - val_accuracy: 0.5983\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.1015 - accuracy: 0.5813 - val_loss: 1.2013 - val_accuracy: 0.5983\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.0879 - accuracy: 0.5617 - val_loss: 1.2037 - val_accuracy: 0.6004\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.1335 - accuracy: 0.5522 - val_loss: 1.2061 - val_accuracy: 0.6004\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0969 - accuracy: 0.5573 - val_loss: 1.2064 - val_accuracy: 0.6026\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.0825 - accuracy: 0.5784 - val_loss: 1.2063 - val_accuracy: 0.6004\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.1019 - accuracy: 0.5493 - val_loss: 1.2065 - val_accuracy: 0.5983\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.1308 - accuracy: 0.5639 - val_loss: 1.2041 - val_accuracy: 0.6070\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.0777 - accuracy: 0.5856 - val_loss: 1.2003 - val_accuracy: 0.6114\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0910 - accuracy: 0.5813 - val_loss: 1.1967 - val_accuracy: 0.6201\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.0410 - accuracy: 0.5871 - val_loss: 1.1917 - val_accuracy: 0.6157\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.0627 - accuracy: 0.5784 - val_loss: 1.1866 - val_accuracy: 0.6092\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.0680 - accuracy: 0.5856 - val_loss: 1.1820 - val_accuracy: 0.6048\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.1173 - accuracy: 0.5885 - val_loss: 1.1784 - val_accuracy: 0.6026\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0707 - accuracy: 0.5900 - val_loss: 1.1755 - val_accuracy: 0.6070\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.0407 - accuracy: 0.6045 - val_loss: 1.1737 - val_accuracy: 0.6092\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.0576 - accuracy: 0.5849 - val_loss: 1.1755 - val_accuracy: 0.6026\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.0653 - accuracy: 0.5871 - val_loss: 1.1780 - val_accuracy: 0.6026\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.0334 - accuracy: 0.5965 - val_loss: 1.1810 - val_accuracy: 0.6026\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.0185 - accuracy: 0.5610 - val_loss: 1.1819 - val_accuracy: 0.5983\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.0530 - accuracy: 0.5849 - val_loss: 1.1815 - val_accuracy: 0.5983\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.0346 - accuracy: 0.5849 - val_loss: 1.1800 - val_accuracy: 0.5939\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.0483 - accuracy: 0.5791 - val_loss: 1.1791 - val_accuracy: 0.5939\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0914 - accuracy: 0.5943 - val_loss: 1.1777 - val_accuracy: 0.5983\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.0247 - accuracy: 0.5718 - val_loss: 1.1774 - val_accuracy: 0.5983\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.0410 - accuracy: 0.6009 - val_loss: 1.1762 - val_accuracy: 0.5983\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.0599 - accuracy: 0.5646 - val_loss: 1.1781 - val_accuracy: 0.5961\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.0355 - accuracy: 0.5987 - val_loss: 1.1779 - val_accuracy: 0.6004\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.0435 - accuracy: 0.5893 - val_loss: 1.1793 - val_accuracy: 0.5983\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.0437 - accuracy: 0.5958 - val_loss: 1.1805 - val_accuracy: 0.6026\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.0084 - accuracy: 0.6103 - val_loss: 1.1812 - val_accuracy: 0.6004\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.0234 - accuracy: 0.5769 - val_loss: 1.1802 - val_accuracy: 0.6004\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.0584 - accuracy: 0.6023 - val_loss: 1.1778 - val_accuracy: 0.6004\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.0347 - accuracy: 0.5951 - val_loss: 1.1727 - val_accuracy: 0.6004\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.0333 - accuracy: 0.5907 - val_loss: 1.1664 - val_accuracy: 0.6092\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.0324 - accuracy: 0.5980 - val_loss: 1.1584 - val_accuracy: 0.6135\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.0897 - accuracy: 0.5951 - val_loss: 1.1529 - val_accuracy: 0.6157\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 0.9813 - accuracy: 0.6168 - val_loss: 1.1499 - val_accuracy: 0.6179\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0519 - accuracy: 0.5864 - val_loss: 1.1479 - val_accuracy: 0.6179\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.9852 - accuracy: 0.6205 - val_loss: 1.1479 - val_accuracy: 0.6223\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.0777 - accuracy: 0.5972 - val_loss: 1.1500 - val_accuracy: 0.6201\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.0594 - accuracy: 0.6118 - val_loss: 1.1517 - val_accuracy: 0.6157\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.0311 - accuracy: 0.5951 - val_loss: 1.1549 - val_accuracy: 0.6157\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.9542 - accuracy: 0.6081 - val_loss: 1.1583 - val_accuracy: 0.6201\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.0393 - accuracy: 0.6103 - val_loss: 1.1591 - val_accuracy: 0.6245\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 0.9999 - accuracy: 0.6023 - val_loss: 1.1578 - val_accuracy: 0.6201\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0260 - accuracy: 0.6132 - val_loss: 1.1567 - val_accuracy: 0.6201\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0245 - accuracy: 0.6132 - val_loss: 1.1550 - val_accuracy: 0.6201\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0268 - accuracy: 0.6168 - val_loss: 1.1536 - val_accuracy: 0.6201\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9859 - accuracy: 0.6118 - val_loss: 1.1531 - val_accuracy: 0.6223\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0406 - accuracy: 0.6154 - val_loss: 1.1538 - val_accuracy: 0.6266\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 1s 564ms/step - loss: 0.9606 - accuracy: 0.6161 - val_loss: 1.1538 - val_accuracy: 0.6201\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.9620 - accuracy: 0.6067 - val_loss: 1.1538 - val_accuracy: 0.6201\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 1s 620ms/step - loss: 1.0317 - accuracy: 0.6030 - val_loss: 1.1523 - val_accuracy: 0.6223\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9970 - accuracy: 0.6241 - val_loss: 1.1510 - val_accuracy: 0.6245\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0067 - accuracy: 0.5987 - val_loss: 1.1495 - val_accuracy: 0.6245\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 1s 641ms/step - loss: 1.0039 - accuracy: 0.6096 - val_loss: 1.1492 - val_accuracy: 0.6201\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.9945 - accuracy: 0.5994 - val_loss: 1.1482 - val_accuracy: 0.6223\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.0081 - accuracy: 0.6060 - val_loss: 1.1480 - val_accuracy: 0.6114\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.0123 - accuracy: 0.6205 - val_loss: 1.1497 - val_accuracy: 0.6114\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.9929 - accuracy: 0.6219 - val_loss: 1.1513 - val_accuracy: 0.6114\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 0.9525 - accuracy: 0.6270 - val_loss: 1.1505 - val_accuracy: 0.6157\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.9873 - accuracy: 0.6161 - val_loss: 1.1498 - val_accuracy: 0.6201\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.0170 - accuracy: 0.6219 - val_loss: 1.1485 - val_accuracy: 0.6201\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.0031 - accuracy: 0.6161 - val_loss: 1.1487 - val_accuracy: 0.6245\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.9798 - accuracy: 0.6321 - val_loss: 1.1474 - val_accuracy: 0.6266\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.0024 - accuracy: 0.6226 - val_loss: 1.1490 - val_accuracy: 0.6201\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.9554 - accuracy: 0.6372 - val_loss: 1.1505 - val_accuracy: 0.6179\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.9414 - accuracy: 0.6226 - val_loss: 1.1505 - val_accuracy: 0.6223\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 0.9618 - accuracy: 0.6306 - val_loss: 1.1487 - val_accuracy: 0.6092\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.0171 - accuracy: 0.6255 - val_loss: 1.1458 - val_accuracy: 0.6092\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.9214 - accuracy: 0.6415 - val_loss: 1.1440 - val_accuracy: 0.6114\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.9530 - accuracy: 0.6379 - val_loss: 1.1395 - val_accuracy: 0.6092\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.9180 - accuracy: 0.6655 - val_loss: 1.1335 - val_accuracy: 0.6135\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.9352 - accuracy: 0.6364 - val_loss: 1.1280 - val_accuracy: 0.6157\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.9186 - accuracy: 0.6350 - val_loss: 1.1247 - val_accuracy: 0.6157\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.9911 - accuracy: 0.6328 - val_loss: 1.1232 - val_accuracy: 0.6157\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.9272 - accuracy: 0.6386 - val_loss: 1.1216 - val_accuracy: 0.6179\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.9607 - accuracy: 0.6393 - val_loss: 1.1208 - val_accuracy: 0.6179\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 0.9122 - accuracy: 0.6524 - val_loss: 1.1211 - val_accuracy: 0.6223\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.9237 - accuracy: 0.6437 - val_loss: 1.1226 - val_accuracy: 0.6179\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 0.9389 - accuracy: 0.6328 - val_loss: 1.1251 - val_accuracy: 0.6223\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.9921 - accuracy: 0.6219 - val_loss: 1.1286 - val_accuracy: 0.6201\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.9498 - accuracy: 0.6379 - val_loss: 1.1332 - val_accuracy: 0.6157\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.9594 - accuracy: 0.6473 - val_loss: 1.1380 - val_accuracy: 0.6179\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.9798 - accuracy: 0.6437 - val_loss: 1.1389 - val_accuracy: 0.6157\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.9684 - accuracy: 0.6176 - val_loss: 1.1383 - val_accuracy: 0.6114\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.9397 - accuracy: 0.6567 - val_loss: 1.1351 - val_accuracy: 0.6201\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.9547 - accuracy: 0.6357 - val_loss: 1.1318 - val_accuracy: 0.6201\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.9375 - accuracy: 0.6567 - val_loss: 1.1285 - val_accuracy: 0.6201\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.9729 - accuracy: 0.6103 - val_loss: 1.1256 - val_accuracy: 0.6157\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.9323 - accuracy: 0.6560 - val_loss: 1.1212 - val_accuracy: 0.6201\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.9698 - accuracy: 0.6226 - val_loss: 1.1190 - val_accuracy: 0.6179\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 0.9386 - accuracy: 0.6459 - val_loss: 1.1173 - val_accuracy: 0.6157\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.9054 - accuracy: 0.6379 - val_loss: 1.1167 - val_accuracy: 0.6201\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9199494c50>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "valFeatures = np.reshape(valFeatures, (458, -1))\n",
        "trainFeatures = np.reshape(trainFeatures, (1378, -1))\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.00005)\n",
        "\n",
        "model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "valconc = [valFeatures, valLabels]\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=50\n",
        ")\n",
        "\n",
        "valData=(valconc ,valTarget)\n",
        "\n",
        "weight = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(trainTarget), y = trainTarget)\n",
        "weight = {i : weight[i] for i in range(6)}\n",
        "\n",
        "# To fit the model, pass a list of inputs arrays\n",
        "model.fit(x=[trainFeatures, trainLabels], y=trainTarget, epochs=300,batch_size=2048,verbose=True, validation_data = valData, class_weight = weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwr2JCRXi8CC"
      },
      "source": [
        "### Adding a Feature Reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtfBA8APoHoI",
        "outputId": "b2e91b45-ba1b-49c8-f9a3-a7db453cc5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import imutils as imutils\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf # machine learning\n",
        "from tqdm import tqdm # make your loops show a smart progress meter \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "from keras.layers import Input, Dense, Concatenate\n",
        "from keras.models import Model\n",
        "# from keras.applications import VGG16\n",
        "import numpy as np\n",
        "\n",
        "# Some random images, labels and target label\n",
        "images = trainX\n",
        "trainLabels = trainXmetadata\n",
        "trainTarget = trainY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "trainFeatures = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w7jm57dt0sE"
      },
      "outputs": [],
      "source": [
        "images = valX\n",
        "valLabels = valXmetadata\n",
        "valTarget = valY\n",
        "\n",
        "# Extract VGG16 features for the images\n",
        "image_input = Input((128, 128, 3))\n",
        "\n",
        "model = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "valFeatures = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awG4v5FfjFCO",
        "outputId": "a7900753-3176-4cf1-d3ec-61ddb704e789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " feature_input (InputLayer)     [(None, 8192)]       0           []                               \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1024)         8389632     ['feature_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 512)          524800      ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " label_input (InputLayer)       [(None, 43)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 555)          0           ['dense_7[0][0]',                \n",
            "                                                                  'label_input[0][0]']            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 128)          71168       ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 64)           8256        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 32)           2080        ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 32)           0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 6)            198         ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,996,134\n",
            "Trainable params: 8,996,134\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Dropout\n",
        "import os\n",
        "import cv2\n",
        "import imutils as imutils\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf # machine learning\n",
        "from tqdm import tqdm # make your loops show a smart progress meter \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "from keras.layers import Input, Dense, Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# Two input layers: one for the image features, one for additional labels\n",
        "feature_input = Input((8192,), name='feature_input')\n",
        "label_input = Input((43, ), name='label_input')\n",
        "\n",
        "\n",
        "dense = Dense(1024,activation='relu')(feature_input)\n",
        "# dense = Dropout(0.6)(dense)\n",
        "dense = Dense(512,activation='relu')(dense)\n",
        "# dense = Dropout(0.6)(dense)\n",
        "concatenate_layer = Concatenate(name='concatenation')([dense, label_input]) \n",
        "dense = Dense(256,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.2)(dense)\n",
        "dense = Dense(128,activation='relu')(concatenate_layer)\n",
        "dense = Dropout(0.2)(dense)\n",
        "dense = Dense(64,activation='relu')(dense)\n",
        "dense = Dropout(0.2)(dense)\n",
        "dense = Dense(32,activation='relu')(dense)\n",
        "flatten=layers.Flatten()(dense) \n",
        "output = Dense(6, name='output_layer', activation='softmax')(flatten)\n",
        "\n",
        "# To define the model, pass list of input layers\n",
        "model = Model(inputs=[feature_input, label_input], outputs=output)\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfC7hslFtw8h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs1jbz25oebZ",
        "outputId": "7d2a9b93-4a06-4dc6-e9a6-0f86f7cd8093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1680 - accuracy: 0.2054 - val_loss: 1.7013 - val_accuracy: 0.2926\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.2703 - accuracy: 0.2293 - val_loss: 1.7246 - val_accuracy: 0.2707\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.2381 - accuracy: 0.2177 - val_loss: 1.7144 - val_accuracy: 0.3122\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1009 - accuracy: 0.1952 - val_loss: 1.7414 - val_accuracy: 0.2402\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.0924 - accuracy: 0.1880 - val_loss: 1.7662 - val_accuracy: 0.2140\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8437 - accuracy: 0.1865 - val_loss: 1.7642 - val_accuracy: 0.2052\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8802 - accuracy: 0.1560 - val_loss: 1.7563 - val_accuracy: 0.1965\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8925 - accuracy: 0.1451 - val_loss: 1.7582 - val_accuracy: 0.1769\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8967 - accuracy: 0.1328 - val_loss: 1.7647 - val_accuracy: 0.1419\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8107 - accuracy: 0.1212 - val_loss: 1.7664 - val_accuracy: 0.1310\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7864 - accuracy: 0.1038 - val_loss: 1.7626 - val_accuracy: 0.1092\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7448 - accuracy: 0.1118 - val_loss: 1.7560 - val_accuracy: 0.1114\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7595 - accuracy: 0.1096 - val_loss: 1.7497 - val_accuracy: 0.1114\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7450 - accuracy: 0.1045 - val_loss: 1.7416 - val_accuracy: 0.1048\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7663 - accuracy: 0.1125 - val_loss: 1.7377 - val_accuracy: 0.1114\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7444 - accuracy: 0.1110 - val_loss: 1.7358 - val_accuracy: 0.1245\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7328 - accuracy: 0.1038 - val_loss: 1.7392 - val_accuracy: 0.1245\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7366 - accuracy: 0.0849 - val_loss: 1.7421 - val_accuracy: 0.1310\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7496 - accuracy: 0.1096 - val_loss: 1.7374 - val_accuracy: 0.1572\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7531 - accuracy: 0.1132 - val_loss: 1.7352 - val_accuracy: 0.1703\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7304 - accuracy: 0.1110 - val_loss: 1.7330 - val_accuracy: 0.1594\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7047 - accuracy: 0.1248 - val_loss: 1.7230 - val_accuracy: 0.1725\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6873 - accuracy: 0.1277 - val_loss: 1.7137 - val_accuracy: 0.1747\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6683 - accuracy: 0.1386 - val_loss: 1.7084 - val_accuracy: 0.2031\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6924 - accuracy: 0.1343 - val_loss: 1.7072 - val_accuracy: 0.2096\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6608 - accuracy: 0.1328 - val_loss: 1.7116 - val_accuracy: 0.2227\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6918 - accuracy: 0.1364 - val_loss: 1.7111 - val_accuracy: 0.2293\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6600 - accuracy: 0.1415 - val_loss: 1.7149 - val_accuracy: 0.2162\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6720 - accuracy: 0.1277 - val_loss: 1.7164 - val_accuracy: 0.2140\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6659 - accuracy: 0.1408 - val_loss: 1.7015 - val_accuracy: 0.2380\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6636 - accuracy: 0.1437 - val_loss: 1.6854 - val_accuracy: 0.2729\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6231 - accuracy: 0.1517 - val_loss: 1.6715 - val_accuracy: 0.3079\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6334 - accuracy: 0.1626 - val_loss: 1.6673 - val_accuracy: 0.2991\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6520 - accuracy: 0.1582 - val_loss: 1.6851 - val_accuracy: 0.2467\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6008 - accuracy: 0.1647 - val_loss: 1.6977 - val_accuracy: 0.1965\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6171 - accuracy: 0.1669 - val_loss: 1.6926 - val_accuracy: 0.2183\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5944 - accuracy: 0.1713 - val_loss: 1.6643 - val_accuracy: 0.2664\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5797 - accuracy: 0.1676 - val_loss: 1.6435 - val_accuracy: 0.2926\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6154 - accuracy: 0.1727 - val_loss: 1.6669 - val_accuracy: 0.2402\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5940 - accuracy: 0.1640 - val_loss: 1.6947 - val_accuracy: 0.2052\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5895 - accuracy: 0.1836 - val_loss: 1.6958 - val_accuracy: 0.2096\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5755 - accuracy: 0.1669 - val_loss: 1.6794 - val_accuracy: 0.2380\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5674 - accuracy: 0.1851 - val_loss: 1.6460 - val_accuracy: 0.2664\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5338 - accuracy: 0.1894 - val_loss: 1.6186 - val_accuracy: 0.3035\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5640 - accuracy: 0.1887 - val_loss: 1.6253 - val_accuracy: 0.2969\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5132 - accuracy: 0.1901 - val_loss: 1.6577 - val_accuracy: 0.2445\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5317 - accuracy: 0.1829 - val_loss: 1.6694 - val_accuracy: 0.2118\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5202 - accuracy: 0.1763 - val_loss: 1.6720 - val_accuracy: 0.2205\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5104 - accuracy: 0.1829 - val_loss: 1.6521 - val_accuracy: 0.2402\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5434 - accuracy: 0.1778 - val_loss: 1.6371 - val_accuracy: 0.2642\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5482 - accuracy: 0.1771 - val_loss: 1.6333 - val_accuracy: 0.2489\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5444 - accuracy: 0.1945 - val_loss: 1.6495 - val_accuracy: 0.2336\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5077 - accuracy: 0.1988 - val_loss: 1.6484 - val_accuracy: 0.2620\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4977 - accuracy: 0.1887 - val_loss: 1.6097 - val_accuracy: 0.3231\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4981 - accuracy: 0.2068 - val_loss: 1.5873 - val_accuracy: 0.3341\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5192 - accuracy: 0.1916 - val_loss: 1.6074 - val_accuracy: 0.3057\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4850 - accuracy: 0.1996 - val_loss: 1.6460 - val_accuracy: 0.2751\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5009 - accuracy: 0.1988 - val_loss: 1.6763 - val_accuracy: 0.2336\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4864 - accuracy: 0.1894 - val_loss: 1.6638 - val_accuracy: 0.2576\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4860 - accuracy: 0.1858 - val_loss: 1.6320 - val_accuracy: 0.2969\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4525 - accuracy: 0.2192 - val_loss: 1.5810 - val_accuracy: 0.3319\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4729 - accuracy: 0.2228 - val_loss: 1.5722 - val_accuracy: 0.3668\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4502 - accuracy: 0.2286 - val_loss: 1.5980 - val_accuracy: 0.3275\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4872 - accuracy: 0.2184 - val_loss: 1.6331 - val_accuracy: 0.2707\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4486 - accuracy: 0.2257 - val_loss: 1.6350 - val_accuracy: 0.2969\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4781 - accuracy: 0.2119 - val_loss: 1.6393 - val_accuracy: 0.2926\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4663 - accuracy: 0.2438 - val_loss: 1.6484 - val_accuracy: 0.3144\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4481 - accuracy: 0.2213 - val_loss: 1.6081 - val_accuracy: 0.3428\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4558 - accuracy: 0.2300 - val_loss: 1.5742 - val_accuracy: 0.3668\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4376 - accuracy: 0.2206 - val_loss: 1.5744 - val_accuracy: 0.3690\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4451 - accuracy: 0.2177 - val_loss: 1.6064 - val_accuracy: 0.3450\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4428 - accuracy: 0.2279 - val_loss: 1.6200 - val_accuracy: 0.3406\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4435 - accuracy: 0.2271 - val_loss: 1.6066 - val_accuracy: 0.3493\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4294 - accuracy: 0.2337 - val_loss: 1.5656 - val_accuracy: 0.3755\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4358 - accuracy: 0.2235 - val_loss: 1.5589 - val_accuracy: 0.3821\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4035 - accuracy: 0.2467 - val_loss: 1.5759 - val_accuracy: 0.3799\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4284 - accuracy: 0.2221 - val_loss: 1.6073 - val_accuracy: 0.3515\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4117 - accuracy: 0.2286 - val_loss: 1.6282 - val_accuracy: 0.3231\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3788 - accuracy: 0.2525 - val_loss: 1.6169 - val_accuracy: 0.3341\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4168 - accuracy: 0.2562 - val_loss: 1.5831 - val_accuracy: 0.3603\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4297 - accuracy: 0.2540 - val_loss: 1.5767 - val_accuracy: 0.3624\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4252 - accuracy: 0.2642 - val_loss: 1.5850 - val_accuracy: 0.3624\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3953 - accuracy: 0.2765 - val_loss: 1.6036 - val_accuracy: 0.3384\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4232 - accuracy: 0.2482 - val_loss: 1.6172 - val_accuracy: 0.3384\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3719 - accuracy: 0.2598 - val_loss: 1.6076 - val_accuracy: 0.3624\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4029 - accuracy: 0.2656 - val_loss: 1.5869 - val_accuracy: 0.3603\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4061 - accuracy: 0.2642 - val_loss: 1.5931 - val_accuracy: 0.3624\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3881 - accuracy: 0.2736 - val_loss: 1.6000 - val_accuracy: 0.3559\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3876 - accuracy: 0.2823 - val_loss: 1.6011 - val_accuracy: 0.3537\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4004 - accuracy: 0.2736 - val_loss: 1.6289 - val_accuracy: 0.3362\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3966 - accuracy: 0.2634 - val_loss: 1.5930 - val_accuracy: 0.3690\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3724 - accuracy: 0.2729 - val_loss: 1.5693 - val_accuracy: 0.3799\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3790 - accuracy: 0.2671 - val_loss: 1.5757 - val_accuracy: 0.3821\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3528 - accuracy: 0.2787 - val_loss: 1.6056 - val_accuracy: 0.3450\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3647 - accuracy: 0.2692 - val_loss: 1.6079 - val_accuracy: 0.3253\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3686 - accuracy: 0.2743 - val_loss: 1.5706 - val_accuracy: 0.3646\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3507 - accuracy: 0.2917 - val_loss: 1.5508 - val_accuracy: 0.3777\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3478 - accuracy: 0.3004 - val_loss: 1.5957 - val_accuracy: 0.3624\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3526 - accuracy: 0.2954 - val_loss: 1.5797 - val_accuracy: 0.3930\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3434 - accuracy: 0.2866 - val_loss: 1.5417 - val_accuracy: 0.4017\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3598 - accuracy: 0.2881 - val_loss: 1.5415 - val_accuracy: 0.3908\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3456 - accuracy: 0.3048 - val_loss: 1.5479 - val_accuracy: 0.3865\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3831 - accuracy: 0.2859 - val_loss: 1.5603 - val_accuracy: 0.3581\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3225 - accuracy: 0.2983 - val_loss: 1.5560 - val_accuracy: 0.3799\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3258 - accuracy: 0.3149 - val_loss: 1.5797 - val_accuracy: 0.3646\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3047 - accuracy: 0.3229 - val_loss: 1.5840 - val_accuracy: 0.3624\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3173 - accuracy: 0.3258 - val_loss: 1.5432 - val_accuracy: 0.3952\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3281 - accuracy: 0.3106 - val_loss: 1.4882 - val_accuracy: 0.4214\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3401 - accuracy: 0.3135 - val_loss: 1.5202 - val_accuracy: 0.4279\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2886 - accuracy: 0.3229 - val_loss: 1.5667 - val_accuracy: 0.3843\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3128 - accuracy: 0.3237 - val_loss: 1.5806 - val_accuracy: 0.3865\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2987 - accuracy: 0.3353 - val_loss: 1.5383 - val_accuracy: 0.3952\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2771 - accuracy: 0.3382 - val_loss: 1.4911 - val_accuracy: 0.4148\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2853 - accuracy: 0.3433 - val_loss: 1.4681 - val_accuracy: 0.4127\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2814 - accuracy: 0.3440 - val_loss: 1.4858 - val_accuracy: 0.4127\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2814 - accuracy: 0.3345 - val_loss: 1.5235 - val_accuracy: 0.3821\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3121 - accuracy: 0.3389 - val_loss: 1.5361 - val_accuracy: 0.3690\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2707 - accuracy: 0.3374 - val_loss: 1.5218 - val_accuracy: 0.3865\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2598 - accuracy: 0.3476 - val_loss: 1.4979 - val_accuracy: 0.4039\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2769 - accuracy: 0.3454 - val_loss: 1.4868 - val_accuracy: 0.4192\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3103 - accuracy: 0.3512 - val_loss: 1.5259 - val_accuracy: 0.3843\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2494 - accuracy: 0.3599 - val_loss: 1.5585 - val_accuracy: 0.3668\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2574 - accuracy: 0.3549 - val_loss: 1.5362 - val_accuracy: 0.3777\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2949 - accuracy: 0.3433 - val_loss: 1.5226 - val_accuracy: 0.3799\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2411 - accuracy: 0.3578 - val_loss: 1.4976 - val_accuracy: 0.3974\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2664 - accuracy: 0.3403 - val_loss: 1.4653 - val_accuracy: 0.4192\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2603 - accuracy: 0.3636 - val_loss: 1.4724 - val_accuracy: 0.4258\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2420 - accuracy: 0.3672 - val_loss: 1.4962 - val_accuracy: 0.4127\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2466 - accuracy: 0.3621 - val_loss: 1.5026 - val_accuracy: 0.4148\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2227 - accuracy: 0.3766 - val_loss: 1.5268 - val_accuracy: 0.4170\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2412 - accuracy: 0.3716 - val_loss: 1.5428 - val_accuracy: 0.3952\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2380 - accuracy: 0.3694 - val_loss: 1.5504 - val_accuracy: 0.3930\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2123 - accuracy: 0.3672 - val_loss: 1.5233 - val_accuracy: 0.4170\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2320 - accuracy: 0.3759 - val_loss: 1.5117 - val_accuracy: 0.4148\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1974 - accuracy: 0.3933 - val_loss: 1.5142 - val_accuracy: 0.4192\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2202 - accuracy: 0.3839 - val_loss: 1.5113 - val_accuracy: 0.4323\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2236 - accuracy: 0.3933 - val_loss: 1.5289 - val_accuracy: 0.4105\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2268 - accuracy: 0.3839 - val_loss: 1.5278 - val_accuracy: 0.4061\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1944 - accuracy: 0.3904 - val_loss: 1.5193 - val_accuracy: 0.4105\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1668 - accuracy: 0.4013 - val_loss: 1.5177 - val_accuracy: 0.4170\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2268 - accuracy: 0.3824 - val_loss: 1.4983 - val_accuracy: 0.4301\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2091 - accuracy: 0.3861 - val_loss: 1.4975 - val_accuracy: 0.4410\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2133 - accuracy: 0.3955 - val_loss: 1.5233 - val_accuracy: 0.3996\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1989 - accuracy: 0.3810 - val_loss: 1.5721 - val_accuracy: 0.3646\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1861 - accuracy: 0.3940 - val_loss: 1.5877 - val_accuracy: 0.3537\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2065 - accuracy: 0.3788 - val_loss: 1.5116 - val_accuracy: 0.4061\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1844 - accuracy: 0.3911 - val_loss: 1.4670 - val_accuracy: 0.4258\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1812 - accuracy: 0.4006 - val_loss: 1.4681 - val_accuracy: 0.4236\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1798 - accuracy: 0.4173 - val_loss: 1.5180 - val_accuracy: 0.4017\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1773 - accuracy: 0.3977 - val_loss: 1.5959 - val_accuracy: 0.3450\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2220 - accuracy: 0.3861 - val_loss: 1.6224 - val_accuracy: 0.3646\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1955 - accuracy: 0.3875 - val_loss: 1.6338 - val_accuracy: 0.4105\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1959 - accuracy: 0.4057 - val_loss: 1.6900 - val_accuracy: 0.4236\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2225 - accuracy: 0.4086 - val_loss: 1.6309 - val_accuracy: 0.3930\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1930 - accuracy: 0.3955 - val_loss: 1.5870 - val_accuracy: 0.4039\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1668 - accuracy: 0.4180 - val_loss: 1.5141 - val_accuracy: 0.4279\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1794 - accuracy: 0.3919 - val_loss: 1.4860 - val_accuracy: 0.4432\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1773 - accuracy: 0.4158 - val_loss: 1.5019 - val_accuracy: 0.4410\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1719 - accuracy: 0.4071 - val_loss: 1.5172 - val_accuracy: 0.4367\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1553 - accuracy: 0.4151 - val_loss: 1.5463 - val_accuracy: 0.4236\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1410 - accuracy: 0.4209 - val_loss: 1.5927 - val_accuracy: 0.4301\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1563 - accuracy: 0.4253 - val_loss: 1.6037 - val_accuracy: 0.4170\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1647 - accuracy: 0.4151 - val_loss: 1.5985 - val_accuracy: 0.4083\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1468 - accuracy: 0.4296 - val_loss: 1.6068 - val_accuracy: 0.4039\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1475 - accuracy: 0.4224 - val_loss: 1.6172 - val_accuracy: 0.4061\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1497 - accuracy: 0.4260 - val_loss: 1.6358 - val_accuracy: 0.4345\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1286 - accuracy: 0.4383 - val_loss: 1.6289 - val_accuracy: 0.4279\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1768 - accuracy: 0.4296 - val_loss: 1.5896 - val_accuracy: 0.4258\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1422 - accuracy: 0.4398 - val_loss: 1.5434 - val_accuracy: 0.4258\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1341 - accuracy: 0.4296 - val_loss: 1.5173 - val_accuracy: 0.4323\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1438 - accuracy: 0.4398 - val_loss: 1.4980 - val_accuracy: 0.4389\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1252 - accuracy: 0.4376 - val_loss: 1.4985 - val_accuracy: 0.4236\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1331 - accuracy: 0.4289 - val_loss: 1.5061 - val_accuracy: 0.4301\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1179 - accuracy: 0.4419 - val_loss: 1.5215 - val_accuracy: 0.4301\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1237 - accuracy: 0.4594 - val_loss: 1.5382 - val_accuracy: 0.4367\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1096 - accuracy: 0.4565 - val_loss: 1.5572 - val_accuracy: 0.4127\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1069 - accuracy: 0.4630 - val_loss: 1.5918 - val_accuracy: 0.4061\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1071 - accuracy: 0.4688 - val_loss: 1.6177 - val_accuracy: 0.4039\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0981 - accuracy: 0.4695 - val_loss: 1.6200 - val_accuracy: 0.3952\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1331 - accuracy: 0.4507 - val_loss: 1.6621 - val_accuracy: 0.4192\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1266 - accuracy: 0.4521 - val_loss: 1.7163 - val_accuracy: 0.4258\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1135 - accuracy: 0.4688 - val_loss: 1.6820 - val_accuracy: 0.4279\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0901 - accuracy: 0.4695 - val_loss: 1.6740 - val_accuracy: 0.4301\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0815 - accuracy: 0.4804 - val_loss: 1.6994 - val_accuracy: 0.4192\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0718 - accuracy: 0.4782 - val_loss: 1.7431 - val_accuracy: 0.4301\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0744 - accuracy: 0.4840 - val_loss: 1.7125 - val_accuracy: 0.4236\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0661 - accuracy: 0.4906 - val_loss: 1.6883 - val_accuracy: 0.4127\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0799 - accuracy: 0.4717 - val_loss: 1.6780 - val_accuracy: 0.4127\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0640 - accuracy: 0.4775 - val_loss: 1.6776 - val_accuracy: 0.4279\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0881 - accuracy: 0.4833 - val_loss: 1.6672 - val_accuracy: 0.4258\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0797 - accuracy: 0.4891 - val_loss: 1.6421 - val_accuracy: 0.4192\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0387 - accuracy: 0.4920 - val_loss: 1.6446 - val_accuracy: 0.4192\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0757 - accuracy: 0.4833 - val_loss: 1.6753 - val_accuracy: 0.4258\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0492 - accuracy: 0.5036 - val_loss: 1.7358 - val_accuracy: 0.4345\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0471 - accuracy: 0.4877 - val_loss: 1.8021 - val_accuracy: 0.4389\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0585 - accuracy: 0.4891 - val_loss: 1.8199 - val_accuracy: 0.4148\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0327 - accuracy: 0.5102 - val_loss: 1.7990 - val_accuracy: 0.4170\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0567 - accuracy: 0.4978 - val_loss: 1.8019 - val_accuracy: 0.4214\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0335 - accuracy: 0.5007 - val_loss: 1.8601 - val_accuracy: 0.4258\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0448 - accuracy: 0.4920 - val_loss: 1.9323 - val_accuracy: 0.4170\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0206 - accuracy: 0.4993 - val_loss: 1.9995 - val_accuracy: 0.4170\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0647 - accuracy: 0.5102 - val_loss: 1.9947 - val_accuracy: 0.4192\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0069 - accuracy: 0.5094 - val_loss: 1.9188 - val_accuracy: 0.4105\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0351 - accuracy: 0.5022 - val_loss: 1.9127 - val_accuracy: 0.4105\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0240 - accuracy: 0.5015 - val_loss: 1.9226 - val_accuracy: 0.4127\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0212 - accuracy: 0.5007 - val_loss: 1.9508 - val_accuracy: 0.4476\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0221 - accuracy: 0.5094 - val_loss: 1.9959 - val_accuracy: 0.4432\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9963 - accuracy: 0.5247 - val_loss: 1.9901 - val_accuracy: 0.4367\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0116 - accuracy: 0.5123 - val_loss: 1.9672 - val_accuracy: 0.4301\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0101 - accuracy: 0.5167 - val_loss: 1.9487 - val_accuracy: 0.4192\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9885 - accuracy: 0.5327 - val_loss: 1.9770 - val_accuracy: 0.4192\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0026 - accuracy: 0.5174 - val_loss: 1.9452 - val_accuracy: 0.4105\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9845 - accuracy: 0.5210 - val_loss: 1.9295 - val_accuracy: 0.4039\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0085 - accuracy: 0.5181 - val_loss: 1.8975 - val_accuracy: 0.4127\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0063 - accuracy: 0.5181 - val_loss: 1.8932 - val_accuracy: 0.4301\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9964 - accuracy: 0.5225 - val_loss: 1.9142 - val_accuracy: 0.4236\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0057 - accuracy: 0.5189 - val_loss: 1.9616 - val_accuracy: 0.4192\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0243 - accuracy: 0.5196 - val_loss: 2.0039 - val_accuracy: 0.4170\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0024 - accuracy: 0.5261 - val_loss: 2.0011 - val_accuracy: 0.4214\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0092 - accuracy: 0.5319 - val_loss: 2.0405 - val_accuracy: 0.4214\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9862 - accuracy: 0.5319 - val_loss: 2.0409 - val_accuracy: 0.4083\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0216 - accuracy: 0.5283 - val_loss: 2.0239 - val_accuracy: 0.4061\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9976 - accuracy: 0.5218 - val_loss: 2.0203 - val_accuracy: 0.4148\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9686 - accuracy: 0.5356 - val_loss: 2.0110 - val_accuracy: 0.4127\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9815 - accuracy: 0.5348 - val_loss: 2.0055 - val_accuracy: 0.4105\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9662 - accuracy: 0.5218 - val_loss: 2.0039 - val_accuracy: 0.4061\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9723 - accuracy: 0.5225 - val_loss: 2.0101 - val_accuracy: 0.4017\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9689 - accuracy: 0.5399 - val_loss: 2.0370 - val_accuracy: 0.4170\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9752 - accuracy: 0.5254 - val_loss: 2.0729 - val_accuracy: 0.4148\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9510 - accuracy: 0.5370 - val_loss: 2.1013 - val_accuracy: 0.4214\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9818 - accuracy: 0.5414 - val_loss: 2.1378 - val_accuracy: 0.4148\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9597 - accuracy: 0.5515 - val_loss: 2.1637 - val_accuracy: 0.4170\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9880 - accuracy: 0.5239 - val_loss: 2.1870 - val_accuracy: 0.4192\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9663 - accuracy: 0.5421 - val_loss: 2.1912 - val_accuracy: 0.4192\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9700 - accuracy: 0.5399 - val_loss: 2.2125 - val_accuracy: 0.4170\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9458 - accuracy: 0.5399 - val_loss: 2.1964 - val_accuracy: 0.4236\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9557 - accuracy: 0.5370 - val_loss: 2.1999 - val_accuracy: 0.4192\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9629 - accuracy: 0.5348 - val_loss: 2.1605 - val_accuracy: 0.4192\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9605 - accuracy: 0.5348 - val_loss: 2.1503 - val_accuracy: 0.4148\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9461 - accuracy: 0.5472 - val_loss: 2.1704 - val_accuracy: 0.3996\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9496 - accuracy: 0.5435 - val_loss: 2.1614 - val_accuracy: 0.4127\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9582 - accuracy: 0.5435 - val_loss: 2.1660 - val_accuracy: 0.4170\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9441 - accuracy: 0.5472 - val_loss: 2.1768 - val_accuracy: 0.4192\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9559 - accuracy: 0.5406 - val_loss: 2.1030 - val_accuracy: 0.4192\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9305 - accuracy: 0.5457 - val_loss: 2.0833 - val_accuracy: 0.4170\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9442 - accuracy: 0.5515 - val_loss: 2.0958 - val_accuracy: 0.4258\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9500 - accuracy: 0.5479 - val_loss: 2.1097 - val_accuracy: 0.4127\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9479 - accuracy: 0.5457 - val_loss: 2.1181 - val_accuracy: 0.4192\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9346 - accuracy: 0.5501 - val_loss: 2.0865 - val_accuracy: 0.4279\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9314 - accuracy: 0.5573 - val_loss: 2.0603 - val_accuracy: 0.4170\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9436 - accuracy: 0.5530 - val_loss: 2.0504 - val_accuracy: 0.4083\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9290 - accuracy: 0.5428 - val_loss: 2.0727 - val_accuracy: 0.4192\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9551 - accuracy: 0.5457 - val_loss: 2.0751 - val_accuracy: 0.4127\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9533 - accuracy: 0.5399 - val_loss: 2.0779 - val_accuracy: 0.4148\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9416 - accuracy: 0.5443 - val_loss: 2.1045 - val_accuracy: 0.4170\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9475 - accuracy: 0.5443 - val_loss: 2.1291 - val_accuracy: 0.4236\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9186 - accuracy: 0.5508 - val_loss: 2.1762 - val_accuracy: 0.4301\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9528 - accuracy: 0.5602 - val_loss: 2.2041 - val_accuracy: 0.4410\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9265 - accuracy: 0.5428 - val_loss: 2.2058 - val_accuracy: 0.4279\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9247 - accuracy: 0.5392 - val_loss: 2.2162 - val_accuracy: 0.4279\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9398 - accuracy: 0.5399 - val_loss: 2.2326 - val_accuracy: 0.4236\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9257 - accuracy: 0.5501 - val_loss: 2.2712 - val_accuracy: 0.4214\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9418 - accuracy: 0.5428 - val_loss: 2.2906 - val_accuracy: 0.4214\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9222 - accuracy: 0.5610 - val_loss: 2.3069 - val_accuracy: 0.4279\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9208 - accuracy: 0.5537 - val_loss: 2.3027 - val_accuracy: 0.4258\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9054 - accuracy: 0.5624 - val_loss: 2.2766 - val_accuracy: 0.4236\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9418 - accuracy: 0.5443 - val_loss: 2.2569 - val_accuracy: 0.4258\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9308 - accuracy: 0.5464 - val_loss: 2.2426 - val_accuracy: 0.4301\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9211 - accuracy: 0.5450 - val_loss: 2.2587 - val_accuracy: 0.4345\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9296 - accuracy: 0.5464 - val_loss: 2.2723 - val_accuracy: 0.4432\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9173 - accuracy: 0.5602 - val_loss: 2.2945 - val_accuracy: 0.4454\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9390 - accuracy: 0.5522 - val_loss: 2.3168 - val_accuracy: 0.4432\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9367 - accuracy: 0.5530 - val_loss: 2.3033 - val_accuracy: 0.4170\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9329 - accuracy: 0.5617 - val_loss: 2.2556 - val_accuracy: 0.4083\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9042 - accuracy: 0.5639 - val_loss: 2.2451 - val_accuracy: 0.4083\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9191 - accuracy: 0.5581 - val_loss: 2.2456 - val_accuracy: 0.4083\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9024 - accuracy: 0.5639 - val_loss: 2.2882 - val_accuracy: 0.4061\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9012 - accuracy: 0.5559 - val_loss: 2.3457 - val_accuracy: 0.4192\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8986 - accuracy: 0.5631 - val_loss: 2.4296 - val_accuracy: 0.4301\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8990 - accuracy: 0.5581 - val_loss: 2.5152 - val_accuracy: 0.4301\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9199 - accuracy: 0.5544 - val_loss: 2.5864 - val_accuracy: 0.4367\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9194 - accuracy: 0.5573 - val_loss: 2.5730 - val_accuracy: 0.4345\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9031 - accuracy: 0.5653 - val_loss: 2.5396 - val_accuracy: 0.4279\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9210 - accuracy: 0.5581 - val_loss: 2.5020 - val_accuracy: 0.4258\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9252 - accuracy: 0.5472 - val_loss: 2.4801 - val_accuracy: 0.4258\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9055 - accuracy: 0.5573 - val_loss: 2.4301 - val_accuracy: 0.4148\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9181 - accuracy: 0.5588 - val_loss: 2.3904 - val_accuracy: 0.4148\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8965 - accuracy: 0.5559 - val_loss: 2.3673 - val_accuracy: 0.4127\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9212 - accuracy: 0.5581 - val_loss: 2.3449 - val_accuracy: 0.4258\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8881 - accuracy: 0.5653 - val_loss: 2.3338 - val_accuracy: 0.4345\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9028 - accuracy: 0.5559 - val_loss: 2.3421 - val_accuracy: 0.4214\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9153 - accuracy: 0.5552 - val_loss: 2.3772 - val_accuracy: 0.4323\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8771 - accuracy: 0.5631 - val_loss: 2.4276 - val_accuracy: 0.4389\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9005 - accuracy: 0.5704 - val_loss: 2.4786 - val_accuracy: 0.4345\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8840 - accuracy: 0.5595 - val_loss: 2.4265 - val_accuracy: 0.4258\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8869 - accuracy: 0.5668 - val_loss: 2.3057 - val_accuracy: 0.3996\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8971 - accuracy: 0.5675 - val_loss: 2.2475 - val_accuracy: 0.3734\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9082 - accuracy: 0.5559 - val_loss: 2.2042 - val_accuracy: 0.3624\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9010 - accuracy: 0.5610 - val_loss: 2.1847 - val_accuracy: 0.3624\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8906 - accuracy: 0.5689 - val_loss: 2.2116 - val_accuracy: 0.3734\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1389ad3710>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "valFeatures = np.reshape(valFeatures, (458, -1))\n",
        "trainFeatures = np.reshape(trainFeatures, (1378, -1))\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.0003)\n",
        "\n",
        "model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "valconc = [valFeatures, valLabels]\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=50\n",
        ")\n",
        "\n",
        "valData=(valconc ,valTarget)\n",
        "\n",
        "weight = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(trainTarget), y = trainTarget)\n",
        "weight = {i : weight[i] for i in range(6)}\n",
        "\n",
        "# To fit the model, pass a list of inputs arrays\n",
        "model.fit(x=[trainFeatures, trainLabels], y=trainTarget, epochs=300,batch_size=1024,verbose=True, validation_data = valData, class_weight = weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvazYYGHzARh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Concatenating Models",
      "provenance": [],
      "authorship_tag": "ABX9TyPkPNwFltA6gNMQU6Ja8q79",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}